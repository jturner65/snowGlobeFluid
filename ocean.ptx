//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-19805474
// Cuda compilation tools, release 7.5, V7.5.16
// Based on LLVM 3.4svn
//

.version 4.3
.target sm_20
.address_size 64

	// .globl	_Z9conjugate6float2
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .func  (.param .align 8 .b8 func_retval0[8]) _Z9conjugate6float2(
	.param .align 8 .b8 _Z9conjugate6float2_param_0[8]
)
{
	.reg .f32 	%f<4>;


	ld.param.f32 	%f1, [_Z9conjugate6float2_param_0];
	ld.param.f32 	%f2, [_Z9conjugate6float2_param_0+4];
	neg.f32 	%f3, %f2;
	st.param.f32	[func_retval0+0], %f1;
	st.param.f32	[func_retval0+4], %f3;
	ret;
}

	// .globl	_Z11complex_expf
.visible .func  (.param .align 8 .b8 func_retval0[8]) _Z11complex_expf(
	.param .b32 _Z11complex_expf_param_0
)
{
	.local .align 4 .b8 	__local_depot1[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .f32 	%f<94>;
	.reg .b32 	%r<184>;
	.reg .b64 	%rd<27>;


	mov.u64 	%rd26, __local_depot1;
	cvta.local.u64 	%SP, %rd26;
	ld.param.f32 	%f36, [_Z11complex_expf_param_0];
	add.u64 	%rd14, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd14;
	abs.f32 	%f1, %f36;
	setp.neu.f32	%p1, %f1, 0f7F800000;
	mov.f32 	%f89, %f36;
	@%p1 bra 	BB1_2;

	mov.f32 	%f37, 0f00000000;
	mul.rn.f32 	%f2, %f36, %f37;
	mov.f32 	%f89, %f2;

BB1_2:
	mov.f32 	%f3, %f89;
	mul.f32 	%f38, %f3, 0f3F22F983;
	cvt.rni.s32.f32	%r173, %f38;
	cvt.rn.f32.s32	%f39, %r173;
	neg.f32 	%f40, %f39;
	mov.f32 	%f41, 0f3FC90FDA;
	fma.rn.f32 	%f42, %f40, %f41, %f3;
	mov.f32 	%f43, 0f33A22168;
	fma.rn.f32 	%f44, %f40, %f43, %f42;
	mov.f32 	%f45, 0f27C234C5;
	fma.rn.f32 	%f83, %f40, %f45, %f44;
	abs.f32 	%f46, %f3;
	add.s64 	%rd2, %rd1, 24;
	setp.leu.f32	%p2, %f46, 0f47CE4780;
	@%p2 bra 	BB1_12;

	mov.b32 	 %r2, %f3;
	shr.u32 	%r3, %r2, 23;
	bfe.u32 	%r76, %r2, 23, 8;
	add.s32 	%r77, %r76, -128;
	shl.b32 	%r78, %r2, 8;
	or.b32  	%r4, %r78, -2147483648;
	shr.u32 	%r5, %r77, 5;
	mov.u32 	%r165, 0;
	mov.u64 	%rd22, __cudart_i2opi_f;
	mov.u32 	%r164, -6;
	mov.u64 	%rd23, %rd1;

BB1_4:
	.pragma "nounroll";
	mov.u64 	%rd4, %rd23;
	ld.const.u32 	%r81, [%rd22];
	// inline asm
	{
	mad.lo.cc.u32   %r79, %r81, %r4, %r165;
	madc.hi.u32     %r165, %r81, %r4,  0;
	}
	// inline asm
	st.local.u32 	[%rd4], %r79;
	add.s64 	%rd5, %rd4, 4;
	add.s64 	%rd22, %rd22, 4;
	add.s32 	%r164, %r164, 1;
	setp.ne.s32	%p3, %r164, 0;
	mov.u64 	%rd23, %rd5;
	@%p3 bra 	BB1_4;

	and.b32  	%r10, %r2, -2147483648;
	st.local.u32 	[%rd2], %r165;
	mov.u32 	%r84, 6;
	sub.s32 	%r85, %r84, %r5;
	mul.wide.s32 	%rd16, %r85, 4;
	add.s64 	%rd7, %rd1, %rd16;
	ld.local.u32 	%r166, [%rd7];
	ld.local.u32 	%r167, [%rd7+-4];
	and.b32  	%r13, %r3, 31;
	setp.eq.s32	%p4, %r13, 0;
	@%p4 bra 	BB1_7;

	mov.u32 	%r86, 32;
	sub.s32 	%r87, %r86, %r13;
	shr.u32 	%r88, %r167, %r87;
	shl.b32 	%r89, %r166, %r13;
	add.s32 	%r166, %r88, %r89;
	ld.local.u32 	%r90, [%rd7+-8];
	shr.u32 	%r91, %r90, %r87;
	shl.b32 	%r92, %r167, %r13;
	add.s32 	%r167, %r91, %r92;

BB1_7:
	shr.u32 	%r93, %r167, 30;
	shl.b32 	%r94, %r166, 2;
	add.s32 	%r168, %r93, %r94;
	shl.b32 	%r19, %r167, 2;
	shr.u32 	%r95, %r168, 31;
	shr.u32 	%r96, %r166, 30;
	add.s32 	%r20, %r95, %r96;
	setp.eq.s32	%p5, %r95, 0;
	mov.u32 	%r169, %r10;
	mov.u32 	%r170, %r19;
	@%p5 bra 	BB1_9;

	not.b32 	%r97, %r168;
	neg.s32 	%r21, %r19;
	setp.eq.s32	%p6, %r19, 0;
	selp.u32	%r98, 1, 0, %p6;
	add.s32 	%r168, %r98, %r97;
	xor.b32  	%r23, %r10, -2147483648;
	mov.u32 	%r169, %r23;
	mov.u32 	%r170, %r21;

BB1_9:
	mov.u32 	%r25, %r169;
	neg.s32 	%r99, %r20;
	setp.eq.s32	%p7, %r10, 0;
	selp.b32	%r173, %r20, %r99, %p7;
	clz.b32 	%r172, %r168;
	setp.eq.s32	%p8, %r172, 0;
	shl.b32 	%r100, %r168, %r172;
	mov.u32 	%r101, 32;
	sub.s32 	%r102, %r101, %r172;
	shr.u32 	%r103, %r170, %r102;
	add.s32 	%r104, %r103, %r100;
	selp.b32	%r29, %r168, %r104, %p8;
	mov.u32 	%r105, -921707870;
	mul.hi.u32 	%r171, %r29, %r105;
	setp.lt.s32	%p9, %r171, 1;
	@%p9 bra 	BB1_11;

	mul.lo.s32 	%r106, %r29, -921707870;
	shr.u32 	%r107, %r106, 31;
	shl.b32 	%r108, %r171, 1;
	add.s32 	%r171, %r107, %r108;
	add.s32 	%r172, %r172, 1;

BB1_11:
	mov.u32 	%r109, 126;
	sub.s32 	%r110, %r109, %r172;
	shl.b32 	%r111, %r110, 23;
	add.s32 	%r112, %r171, 1;
	shr.u32 	%r113, %r112, 7;
	add.s32 	%r114, %r113, 1;
	shr.u32 	%r115, %r114, 1;
	add.s32 	%r116, %r115, %r111;
	or.b32  	%r117, %r116, %r25;
	mov.b32 	 %f83, %r117;

BB1_12:
	mul.rn.f32 	%f7, %f83, %f83;
	add.s32 	%r36, %r173, 1;
	and.b32  	%r37, %r36, 1;
	setp.eq.s32	%p10, %r37, 0;
	@%p10 bra 	BB1_14;

	mov.f32 	%f47, 0fBAB6061A;
	mov.f32 	%f48, 0f37CCF5CE;
	fma.rn.f32 	%f84, %f48, %f7, %f47;
	bra.uni 	BB1_15;

BB1_14:
	mov.f32 	%f49, 0f3C08839E;
	mov.f32 	%f50, 0fB94CA1F9;
	fma.rn.f32 	%f84, %f50, %f7, %f49;

BB1_15:
	@%p10 bra 	BB1_17;

	mov.f32 	%f51, 0f3D2AAAA5;
	fma.rn.f32 	%f52, %f84, %f7, %f51;
	mov.f32 	%f53, 0fBF000000;
	fma.rn.f32 	%f85, %f52, %f7, %f53;
	bra.uni 	BB1_18;

BB1_17:
	mov.f32 	%f54, 0fBE2AAAA3;
	fma.rn.f32 	%f55, %f84, %f7, %f54;
	mov.f32 	%f56, 0f00000000;
	fma.rn.f32 	%f85, %f55, %f7, %f56;

BB1_18:
	fma.rn.f32 	%f86, %f85, %f83, %f83;
	@%p10 bra 	BB1_20;

	mov.f32 	%f57, 0f3F800000;
	fma.rn.f32 	%f86, %f85, %f7, %f57;

BB1_20:
	and.b32  	%r118, %r36, 2;
	setp.eq.s32	%p13, %r118, 0;
	@%p13 bra 	BB1_22;

	mov.f32 	%f58, 0f00000000;
	mov.f32 	%f59, 0fBF800000;
	fma.rn.f32 	%f86, %f86, %f59, %f58;

BB1_22:
	mov.f32 	%f88, %f36;
	@%p1 bra 	BB1_24;

	mov.f32 	%f60, 0f00000000;
	mul.rn.f32 	%f88, %f36, %f60;

BB1_24:
	mul.f32 	%f61, %f88, 0f3F22F983;
	cvt.rni.s32.f32	%r183, %f61;
	cvt.rn.f32.s32	%f62, %r183;
	neg.f32 	%f63, %f62;
	fma.rn.f32 	%f65, %f63, %f41, %f88;
	fma.rn.f32 	%f67, %f63, %f43, %f65;
	fma.rn.f32 	%f90, %f63, %f45, %f67;
	abs.f32 	%f69, %f88;
	setp.leu.f32	%p15, %f69, 0f47CE4780;
	@%p15 bra 	BB1_34;

	mov.b32 	 %r39, %f88;
	shr.u32 	%r40, %r39, 23;
	bfe.u32 	%r121, %r39, 23, 8;
	add.s32 	%r122, %r121, -128;
	shl.b32 	%r123, %r39, 8;
	or.b32  	%r41, %r123, -2147483648;
	shr.u32 	%r42, %r122, 5;
	cvta.to.local.u64 	%rd25, %rd14;
	mov.u32 	%r175, 0;
	mov.u64 	%rd24, __cudart_i2opi_f;
	mov.u32 	%r174, -6;

BB1_26:
	.pragma "nounroll";
	ld.const.u32 	%r126, [%rd24];
	// inline asm
	{
	mad.lo.cc.u32   %r124, %r126, %r41, %r175;
	madc.hi.u32     %r175, %r126, %r41,  0;
	}
	// inline asm
	st.local.u32 	[%rd25], %r124;
	add.s64 	%rd25, %rd25, 4;
	add.s64 	%rd24, %rd24, 4;
	add.s32 	%r174, %r174, 1;
	setp.ne.s32	%p16, %r174, 0;
	@%p16 bra 	BB1_26;

	and.b32  	%r47, %r39, -2147483648;
	st.local.u32 	[%rd2], %r175;
	mov.u32 	%r129, 6;
	sub.s32 	%r130, %r129, %r42;
	cvta.to.local.u64 	%rd20, %rd14;
	mul.wide.s32 	%rd21, %r130, 4;
	add.s64 	%rd13, %rd20, %rd21;
	ld.local.u32 	%r176, [%rd13];
	ld.local.u32 	%r177, [%rd13+-4];
	and.b32  	%r50, %r40, 31;
	setp.eq.s32	%p17, %r50, 0;
	@%p17 bra 	BB1_29;

	mov.u32 	%r131, 32;
	sub.s32 	%r132, %r131, %r50;
	shr.u32 	%r133, %r177, %r132;
	shl.b32 	%r134, %r176, %r50;
	add.s32 	%r176, %r133, %r134;
	ld.local.u32 	%r135, [%rd13+-8];
	shr.u32 	%r136, %r135, %r132;
	shl.b32 	%r137, %r177, %r50;
	add.s32 	%r177, %r136, %r137;

BB1_29:
	shr.u32 	%r138, %r177, 30;
	shl.b32 	%r139, %r176, 2;
	add.s32 	%r178, %r138, %r139;
	shl.b32 	%r56, %r177, 2;
	shr.u32 	%r140, %r178, 31;
	shr.u32 	%r141, %r176, 30;
	add.s32 	%r57, %r140, %r141;
	setp.eq.s32	%p18, %r140, 0;
	mov.u32 	%r179, %r47;
	mov.u32 	%r180, %r56;
	@%p18 bra 	BB1_31;

	not.b32 	%r142, %r178;
	neg.s32 	%r58, %r56;
	setp.eq.s32	%p19, %r56, 0;
	selp.u32	%r143, 1, 0, %p19;
	add.s32 	%r178, %r143, %r142;
	xor.b32  	%r60, %r47, -2147483648;
	mov.u32 	%r179, %r60;
	mov.u32 	%r180, %r58;

BB1_31:
	mov.u32 	%r62, %r179;
	neg.s32 	%r144, %r57;
	setp.eq.s32	%p20, %r47, 0;
	selp.b32	%r183, %r57, %r144, %p20;
	clz.b32 	%r182, %r178;
	setp.eq.s32	%p21, %r182, 0;
	shl.b32 	%r145, %r178, %r182;
	mov.u32 	%r146, 32;
	sub.s32 	%r147, %r146, %r182;
	shr.u32 	%r148, %r180, %r147;
	add.s32 	%r149, %r148, %r145;
	selp.b32	%r66, %r178, %r149, %p21;
	mov.u32 	%r150, -921707870;
	mul.hi.u32 	%r181, %r66, %r150;
	setp.lt.s32	%p22, %r181, 1;
	@%p22 bra 	BB1_33;

	mul.lo.s32 	%r151, %r66, -921707870;
	shr.u32 	%r152, %r151, 31;
	shl.b32 	%r153, %r181, 1;
	add.s32 	%r181, %r152, %r153;
	add.s32 	%r182, %r182, 1;

BB1_33:
	mov.u32 	%r154, 126;
	sub.s32 	%r155, %r154, %r182;
	shl.b32 	%r156, %r155, 23;
	add.s32 	%r157, %r181, 1;
	shr.u32 	%r158, %r157, 7;
	add.s32 	%r159, %r158, 1;
	shr.u32 	%r160, %r159, 1;
	add.s32 	%r161, %r160, %r156;
	or.b32  	%r162, %r161, %r62;
	mov.b32 	 %f90, %r162;

BB1_34:
	mul.rn.f32 	%f24, %f90, %f90;
	and.b32  	%r73, %r183, 1;
	setp.eq.s32	%p23, %r73, 0;
	@%p23 bra 	BB1_36;

	mov.f32 	%f70, 0fBAB6061A;
	mov.f32 	%f71, 0f37CCF5CE;
	fma.rn.f32 	%f91, %f71, %f24, %f70;
	bra.uni 	BB1_37;

BB1_36:
	mov.f32 	%f72, 0f3C08839E;
	mov.f32 	%f73, 0fB94CA1F9;
	fma.rn.f32 	%f91, %f73, %f24, %f72;

BB1_37:
	@%p23 bra 	BB1_39;

	mov.f32 	%f74, 0f3D2AAAA5;
	fma.rn.f32 	%f75, %f91, %f24, %f74;
	mov.f32 	%f76, 0fBF000000;
	fma.rn.f32 	%f92, %f75, %f24, %f76;
	bra.uni 	BB1_40;

BB1_39:
	mov.f32 	%f77, 0fBE2AAAA3;
	fma.rn.f32 	%f78, %f91, %f24, %f77;
	mov.f32 	%f79, 0f00000000;
	fma.rn.f32 	%f92, %f78, %f24, %f79;

BB1_40:
	fma.rn.f32 	%f93, %f92, %f90, %f90;
	@%p23 bra 	BB1_42;

	mov.f32 	%f80, 0f3F800000;
	fma.rn.f32 	%f93, %f92, %f24, %f80;

BB1_42:
	and.b32  	%r163, %r183, 2;
	setp.eq.s32	%p26, %r163, 0;
	@%p26 bra 	BB1_44;

	mov.f32 	%f81, 0f00000000;
	mov.f32 	%f82, 0fBF800000;
	fma.rn.f32 	%f93, %f93, %f82, %f81;

BB1_44:
	st.param.f32	[func_retval0+0], %f86;
	st.param.f32	[func_retval0+4], %f93;
	ret;
}

	// .globl	_Z11complex_add6float2S_
.visible .func  (.param .align 8 .b8 func_retval0[8]) _Z11complex_add6float2S_(
	.param .align 8 .b8 _Z11complex_add6float2S__param_0[8],
	.param .align 8 .b8 _Z11complex_add6float2S__param_1[8]
)
{
	.reg .f32 	%f<7>;


	ld.param.f32 	%f1, [_Z11complex_add6float2S__param_0+4];
	ld.param.f32 	%f2, [_Z11complex_add6float2S__param_0];
	ld.param.f32 	%f3, [_Z11complex_add6float2S__param_1+4];
	ld.param.f32 	%f4, [_Z11complex_add6float2S__param_1];
	add.f32 	%f5, %f2, %f4;
	add.f32 	%f6, %f1, %f3;
	st.param.f32	[func_retval0+0], %f5;
	st.param.f32	[func_retval0+4], %f6;
	ret;
}

	// .globl	_Z9interp2F26float2S_f
.visible .func  (.param .align 8 .b8 func_retval0[8]) _Z9interp2F26float2S_f(
	.param .align 8 .b8 _Z9interp2F26float2S_f_param_0[8],
	.param .align 8 .b8 _Z9interp2F26float2S_f_param_1[8],
	.param .b32 _Z9interp2F26float2S_f_param_2
)
{
	.reg .f32 	%f<10>;


	ld.param.f32 	%f1, [_Z9interp2F26float2S_f_param_0+4];
	ld.param.f32 	%f2, [_Z9interp2F26float2S_f_param_0];
	ld.param.f32 	%f3, [_Z9interp2F26float2S_f_param_1+4];
	ld.param.f32 	%f4, [_Z9interp2F26float2S_f_param_1];
	ld.param.f32 	%f5, [_Z9interp2F26float2S_f_param_2];
	sub.f32 	%f6, %f4, %f2;
	fma.rn.f32 	%f7, %f6, %f5, %f2;
	sub.f32 	%f8, %f3, %f1;
	fma.rn.f32 	%f9, %f8, %f5, %f1;
	st.param.f32	[func_retval0+0], %f7;
	st.param.f32	[func_retval0+4], %f9;
	ret;
}

	// .globl	_Z12complex_mult6float2S_
.visible .func  (.param .align 8 .b8 func_retval0[8]) _Z12complex_mult6float2S_(
	.param .align 8 .b8 _Z12complex_mult6float2S__param_0[8],
	.param .align 8 .b8 _Z12complex_mult6float2S__param_1[8]
)
{
	.reg .f32 	%f<10>;


	ld.param.f32 	%f1, [_Z12complex_mult6float2S__param_0+4];
	ld.param.f32 	%f2, [_Z12complex_mult6float2S__param_0];
	ld.param.f32 	%f3, [_Z12complex_mult6float2S__param_1+4];
	ld.param.f32 	%f4, [_Z12complex_mult6float2S__param_1];
	mul.f32 	%f5, %f2, %f4;
	mul.f32 	%f6, %f1, %f3;
	sub.f32 	%f7, %f5, %f6;
	mul.f32 	%f8, %f1, %f4;
	fma.rn.f32 	%f9, %f2, %f3, %f8;
	st.param.f32	[func_retval0+0], %f7;
	st.param.f32	[func_retval0+4], %f9;
	ret;
}

	// .globl	buildFrequencyDataKernel
.visible .entry buildFrequencyDataKernel(
	.param .u64 buildFrequencyDataKernel_param_0,
	.param .u64 buildFrequencyDataKernel_param_1,
	.param .u64 buildFrequencyDataKernel_param_2,
	.param .u32 buildFrequencyDataKernel_param_3,
	.param .u32 buildFrequencyDataKernel_param_4,
	.param .u32 buildFrequencyDataKernel_param_5,
	.param .u32 buildFrequencyDataKernel_param_6,
	.param .f32 buildFrequencyDataKernel_param_7,
	.param .f32 buildFrequencyDataKernel_param_8
)
{
	.local .align 4 .b8 	__local_depot5[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<100>;
	.reg .f32 	%f<446>;
	.reg .b32 	%r<425>;
	.reg .b64 	%rd<74>;


	mov.u64 	%rd73, __local_depot5;
	cvta.local.u64 	%SP, %rd73;
	ld.param.u64 	%rd27, [buildFrequencyDataKernel_param_1];
	ld.param.u64 	%rd28, [buildFrequencyDataKernel_param_2];
	ld.param.u32 	%r149, [buildFrequencyDataKernel_param_3];
	ld.param.u32 	%r150, [buildFrequencyDataKernel_param_4];
	ld.param.u32 	%r151, [buildFrequencyDataKernel_param_5];
	ld.param.f32 	%f114, [buildFrequencyDataKernel_param_7];
	mov.u32 	%r152, %ntid.x;
	mov.u32 	%r153, %ctaid.x;
	mov.u32 	%r154, %tid.x;
	mad.lo.s32 	%r1, %r152, %r153, %r154;
	mov.u32 	%r155, %ntid.y;
	mov.u32 	%r156, %ctaid.y;
	mov.u32 	%r157, %tid.y;
	mad.lo.s32 	%r2, %r155, %r156, %r157;
	cvt.rn.f32.u32	%f116, %r1;
	cvt.rn.f32.u32	%f117, %r150;
	div.rn.f32 	%f118, %f116, %f117;
	cvt.rn.f32.u32	%f119, %r2;
	cvt.rn.f32.u32	%f120, %r151;
	div.rn.f32 	%f121, %f119, %f120;
	fma.rn.f32 	%f1, %f118, 0f40000000, 0fBF800000;
	fma.rn.f32 	%f2, %f121, 0f40000000, 0fBF800000;
	setp.lt.u32	%p1, %r1, %r150;
	setp.lt.u32	%p2, %r2, %r151;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB5_114;
	bra.uni 	BB5_1;

BB5_1:
	add.s32 	%r158, %r149, -1;
	rem.u32 	%r159, %r1, %r158;
	rem.u32 	%r160, %r2, %r158;
	add.s32 	%r161, %r160, 1;
	add.s32 	%r162, %r159, 1;
	cvta.to.global.u64 	%rd29, %rd27;
	mul.wide.u32 	%rd30, %r162, 4;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.f32 	%f122, [%rd31];
	setp.lt.f32	%p4, %f122, %f114;
	selp.f32	%f3, %f114, %f122, %p4;
	cvta.to.global.u64 	%rd32, %rd28;
	mul.wide.u32 	%rd33, %r161, 4;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.f32 	%f123, [%rd34];
	setp.lt.f32	%p5, %f123, %f114;
	selp.f32	%f4, %f114, %f123, %p5;
	add.f32 	%f5, %f3, 0f3F800000;
	setp.gt.f32	%p6, %f5, 0f00000000;
	setp.lt.f32	%p7, %f5, 0f7F800000;
	and.pred  	%p8, %p6, %p7;
	@%p8 bra 	BB5_3;
	bra.uni 	BB5_2;

BB5_3:
	setp.lt.f32	%p9, %f5, 0f00800000;
	mul.f32 	%f126, %f5, 0f4B800000;
	selp.f32	%f127, %f126, %f5, %p9;
	selp.f32	%f128, 0fC3170000, 0fC2FE0000, %p9;
	mov.b32 	 %r163, %f127;
	and.b32  	%r164, %r163, 8388607;
	or.b32  	%r165, %r164, 1065353216;
	mov.b32 	 %f129, %r165;
	shr.u32 	%r166, %r163, 23;
	cvt.rn.f32.u32	%f130, %r166;
	add.f32 	%f131, %f128, %f130;
	setp.gt.f32	%p10, %f129, 0f3FAE147B;
	mul.f32 	%f132, %f129, 0f3F000000;
	add.f32 	%f133, %f131, 0f3F800000;
	selp.f32	%f134, %f132, %f129, %p10;
	selp.f32	%f135, %f133, %f131, %p10;
	add.f32 	%f125, %f134, 0f3F800000;
	add.f32 	%f136, %f134, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f124,%f125;
	// inline asm
	mul.f32 	%f137, %f136, %f136;
	neg.f32 	%f138, %f137;
	mul.rn.f32 	%f139, %f124, %f138;
	add.rn.f32 	%f140, %f136, %f139;
	mul.f32 	%f141, %f140, %f140;
	mov.f32 	%f142, 0f3C4C6A36;
	mov.f32 	%f143, 0f3B1E94E6;
	fma.rn.f32 	%f144, %f143, %f141, %f142;
	mov.f32 	%f145, 0f3DAAAB1A;
	fma.rn.f32 	%f146, %f144, %f141, %f145;
	mul.f32 	%f147, %f141, %f146;
	fma.rn.f32 	%f148, %f147, %f140, %f139;
	add.f32 	%f149, %f136, %f148;
	mov.f32 	%f150, 0f3F317218;
	fma.rn.f32 	%f422, %f135, %f150, %f149;
	bra.uni 	BB5_4;

BB5_2:
	lg2.approx.f32 	%f422, %f5;

BB5_4:
	mul.f32 	%f155, %f422, 0f3FB8AA3B;
	cvt.rni.s64.f32	%rd35, %f155;
	add.s64 	%rd36, %rd35, -1;
	cvt.rn.f32.s64	%f9, %rd36;
	mov.f32 	%f156, 0f40000000;
	abs.f32 	%f10, %f156;
	setp.lt.f32	%p11, %f10, 0f00800000;
	mul.f32 	%f157, %f10, 0f4B800000;
	selp.f32	%f158, 0fC3170000, 0fC2FE0000, %p11;
	selp.f32	%f159, %f157, %f10, %p11;
	mov.b32 	 %r167, %f159;
	and.b32  	%r168, %r167, 8388607;
	or.b32  	%r169, %r168, 1065353216;
	mov.b32 	 %f160, %r169;
	shr.u32 	%r170, %r167, 23;
	cvt.rn.f32.u32	%f161, %r170;
	add.f32 	%f162, %f158, %f161;
	setp.gt.f32	%p12, %f160, 0f3FB504F3;
	mul.f32 	%f163, %f160, 0f3F000000;
	add.f32 	%f164, %f162, 0f3F800000;
	selp.f32	%f165, %f163, %f160, %p12;
	selp.f32	%f166, %f164, %f162, %p12;
	add.f32 	%f11, %f165, 0fBF800000;
	add.f32 	%f152, %f165, 0f3F800000;
	// inline asm
	rcp.approx.ftz.f32 %f151,%f152;
	// inline asm
	add.f32 	%f13, %f11, %f11;
	mul.f32 	%f167, %f151, %f13;
	mul.f32 	%f168, %f167, %f167;
	mov.f32 	%f169, 0f3C4CAF63;
	mov.f32 	%f170, 0f3B18F0FE;
	fma.rn.f32 	%f171, %f170, %f168, %f169;
	mov.f32 	%f172, 0f3DAAAABD;
	fma.rn.f32 	%f173, %f171, %f168, %f172;
	mul.rn.f32 	%f174, %f173, %f168;
	mul.rn.f32 	%f175, %f174, %f167;
	sub.f32 	%f176, %f11, %f167;
	neg.f32 	%f177, %f167;
	add.f32 	%f178, %f176, %f176;
	fma.rn.f32 	%f179, %f177, %f11, %f178;
	mul.rn.f32 	%f180, %f151, %f179;
	add.f32 	%f181, %f175, %f167;
	sub.f32 	%f182, %f167, %f181;
	add.f32 	%f183, %f175, %f182;
	add.f32 	%f184, %f180, %f183;
	add.f32 	%f185, %f181, %f184;
	sub.f32 	%f186, %f181, %f185;
	add.f32 	%f187, %f184, %f186;
	mov.f32 	%f188, 0f3F317200;
	mul.rn.f32 	%f14, %f166, %f188;
	mov.f32 	%f189, 0f35BFBE8E;
	mul.rn.f32 	%f15, %f166, %f189;
	add.f32 	%f190, %f14, %f185;
	sub.f32 	%f191, %f14, %f190;
	add.f32 	%f192, %f185, %f191;
	add.f32 	%f193, %f187, %f192;
	add.f32 	%f194, %f15, %f193;
	add.f32 	%f195, %f190, %f194;
	sub.f32 	%f196, %f190, %f195;
	add.f32 	%f197, %f194, %f196;
	abs.f32 	%f16, %f9;
	setp.gt.f32	%p13, %f16, 0f77F684DF;
	mul.f32 	%f198, %f9, 0f39000000;
	selp.f32	%f199, %f198, %f9, %p13;
	mul.rn.f32 	%f200, %f199, %f195;
	neg.f32 	%f201, %f200;
	fma.rn.f32 	%f202, %f199, %f195, %f201;
	fma.rn.f32 	%f203, %f199, %f197, %f202;
	mov.f32 	%f204, 0f00000000;
	fma.rn.f32 	%f205, %f204, %f195, %f203;
	add.rn.f32 	%f206, %f200, %f205;
	neg.f32 	%f207, %f206;
	add.rn.f32 	%f208, %f200, %f207;
	add.rn.f32 	%f209, %f208, %f205;
	mov.b32 	 %r171, %f206;
	setp.eq.s32	%p14, %r171, 1118925336;
	add.s32 	%r172, %r171, -1;
	mov.b32 	 %f210, %r172;
	add.f32 	%f211, %f209, 0f37000000;
	selp.f32	%f212, %f210, %f206, %p14;
	selp.f32	%f17, %f211, %f209, %p14;
	mul.f32 	%f213, %f212, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f214, %f213;
	mov.f32 	%f215, 0fBF317200;
	fma.rn.f32 	%f216, %f214, %f215, %f212;
	mov.f32 	%f217, 0fB5BFBE8E;
	fma.rn.f32 	%f218, %f214, %f217, %f216;
	mul.f32 	%f154, %f218, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f153,%f154;
	// inline asm
	add.f32 	%f219, %f214, 0f00000000;
	ex2.approx.f32 	%f220, %f219;
	mul.f32 	%f221, %f153, %f220;
	setp.lt.f32	%p15, %f212, 0fC2D20000;
	selp.f32	%f222, 0f00000000, %f221, %p15;
	setp.gt.f32	%p16, %f212, 0f42D20000;
	selp.f32	%f423, 0f7F800000, %f222, %p16;
	setp.eq.f32	%p17, %f423, 0f7F800000;
	@%p17 bra 	BB5_6;

	fma.rn.f32 	%f423, %f423, %f17, %f423;

BB5_6:
	add.f32 	%f223, %f10, %f16;
	mov.b32 	 %r173, %f223;
	setp.lt.s32	%p18, %r173, 2139095040;
	@%p18 bra 	BB5_13;

	setp.gtu.f32	%p19, %f10, 0f7F800000;
	setp.gtu.f32	%p20, %f16, 0f7F800000;
	or.pred  	%p21, %p19, %p20;
	@%p21 bra 	BB5_12;
	bra.uni 	BB5_8;

BB5_12:
	add.f32 	%f423, %f9, 0f40000000;
	bra.uni 	BB5_13;

BB5_8:
	setp.eq.f32	%p22, %f16, 0f7F800000;
	@%p22 bra 	BB5_11;
	bra.uni 	BB5_9;

BB5_11:
	setp.gt.f32	%p25, %f10, 0f3F800000;
	selp.b32	%r174, 2139095040, 0, %p25;
	xor.b32  	%r175, %r174, 2139095040;
	setp.lt.f32	%p26, %f9, 0f00000000;
	selp.b32	%r176, %r175, %r174, %p26;
	mov.b32 	 %f423, %r176;
	bra.uni 	BB5_13;

BB5_9:
	setp.neu.f32	%p23, %f10, 0f7F800000;
	@%p23 bra 	BB5_13;

	setp.ltu.f32	%p24, %f9, 0f00000000;
	selp.f32	%f423, 0f00000000, 0f7F800000, %p24;

BB5_13:
	setp.eq.f32	%p27, %f9, 0f00000000;
	selp.f32	%f224, 0f3F800000, %f423, %p27;
	div.rn.f32 	%f25, %f3, %f224;
	add.f32 	%f26, %f4, 0f3F800000;
	setp.gt.f32	%p28, %f26, 0f00000000;
	setp.lt.f32	%p29, %f26, 0f7F800000;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	BB5_15;
	bra.uni 	BB5_14;

BB5_15:
	setp.lt.f32	%p31, %f26, 0f00800000;
	mul.f32 	%f227, %f26, 0f4B800000;
	selp.f32	%f228, %f227, %f26, %p31;
	selp.f32	%f229, 0fC3170000, 0fC2FE0000, %p31;
	mov.b32 	 %r177, %f228;
	and.b32  	%r178, %r177, 8388607;
	or.b32  	%r179, %r178, 1065353216;
	mov.b32 	 %f230, %r179;
	shr.u32 	%r180, %r177, 23;
	cvt.rn.f32.u32	%f231, %r180;
	add.f32 	%f232, %f229, %f231;
	setp.gt.f32	%p32, %f230, 0f3FAE147B;
	mul.f32 	%f233, %f230, 0f3F000000;
	add.f32 	%f234, %f232, 0f3F800000;
	selp.f32	%f235, %f233, %f230, %p32;
	selp.f32	%f236, %f234, %f232, %p32;
	add.f32 	%f226, %f235, 0f3F800000;
	add.f32 	%f237, %f235, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f225,%f226;
	// inline asm
	mul.f32 	%f238, %f237, %f237;
	neg.f32 	%f239, %f238;
	mul.rn.f32 	%f240, %f225, %f239;
	add.rn.f32 	%f241, %f237, %f240;
	mul.f32 	%f242, %f241, %f241;
	mov.f32 	%f243, 0f3C4C6A36;
	mov.f32 	%f244, 0f3B1E94E6;
	fma.rn.f32 	%f245, %f244, %f242, %f243;
	mov.f32 	%f246, 0f3DAAAB1A;
	fma.rn.f32 	%f247, %f245, %f242, %f246;
	mul.f32 	%f248, %f242, %f247;
	fma.rn.f32 	%f249, %f248, %f241, %f240;
	add.f32 	%f250, %f237, %f249;
	mov.f32 	%f251, 0f3F317218;
	fma.rn.f32 	%f424, %f236, %f251, %f250;
	bra.uni 	BB5_16;

BB5_14:
	lg2.approx.f32 	%f424, %f26;

BB5_16:
	mov.f32 	%f420, 0fB5BFBE8E;
	mov.f32 	%f419, 0fBF317200;
	mov.f32 	%f418, 0f3DAAAABD;
	mov.f32 	%f417, 0f3C4CAF63;
	mov.f32 	%f416, 0f3B18F0FE;
	mul.f32 	%f256, %f424, 0f3FB8AA3B;
	cvt.rni.s64.f32	%rd37, %f256;
	add.s64 	%rd38, %rd37, -1;
	cvt.rn.f32.s64	%f30, %rd38;
	// inline asm
	rcp.approx.ftz.f32 %f252,%f152;
	// inline asm
	mul.f32 	%f257, %f252, %f13;
	mul.f32 	%f258, %f257, %f257;
	fma.rn.f32 	%f261, %f416, %f258, %f417;
	fma.rn.f32 	%f263, %f261, %f258, %f418;
	mul.rn.f32 	%f264, %f263, %f258;
	mul.rn.f32 	%f265, %f264, %f257;
	sub.f32 	%f266, %f11, %f257;
	neg.f32 	%f267, %f257;
	add.f32 	%f268, %f266, %f266;
	fma.rn.f32 	%f269, %f267, %f11, %f268;
	mul.rn.f32 	%f270, %f252, %f269;
	add.f32 	%f271, %f265, %f257;
	sub.f32 	%f272, %f257, %f271;
	add.f32 	%f273, %f265, %f272;
	add.f32 	%f274, %f270, %f273;
	add.f32 	%f275, %f271, %f274;
	sub.f32 	%f276, %f271, %f275;
	add.f32 	%f277, %f274, %f276;
	add.f32 	%f278, %f14, %f275;
	sub.f32 	%f279, %f14, %f278;
	add.f32 	%f280, %f275, %f279;
	add.f32 	%f281, %f277, %f280;
	add.f32 	%f282, %f15, %f281;
	add.f32 	%f283, %f278, %f282;
	sub.f32 	%f284, %f278, %f283;
	add.f32 	%f285, %f282, %f284;
	abs.f32 	%f31, %f30;
	setp.gt.f32	%p33, %f31, 0f77F684DF;
	mul.f32 	%f286, %f30, 0f39000000;
	selp.f32	%f287, %f286, %f30, %p33;
	mul.rn.f32 	%f288, %f287, %f283;
	neg.f32 	%f289, %f288;
	fma.rn.f32 	%f290, %f287, %f283, %f289;
	fma.rn.f32 	%f291, %f287, %f285, %f290;
	fma.rn.f32 	%f293, %f204, %f283, %f291;
	add.rn.f32 	%f294, %f288, %f293;
	neg.f32 	%f295, %f294;
	add.rn.f32 	%f296, %f288, %f295;
	add.rn.f32 	%f297, %f296, %f293;
	mov.b32 	 %r181, %f294;
	setp.eq.s32	%p34, %r181, 1118925336;
	add.s32 	%r182, %r181, -1;
	mov.b32 	 %f298, %r182;
	add.f32 	%f299, %f297, 0f37000000;
	selp.f32	%f300, %f298, %f294, %p34;
	selp.f32	%f32, %f299, %f297, %p34;
	mul.f32 	%f301, %f300, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f302, %f301;
	fma.rn.f32 	%f304, %f302, %f419, %f300;
	fma.rn.f32 	%f306, %f302, %f420, %f304;
	mul.f32 	%f255, %f306, 0f3FB8AA3B;
	// inline asm
	ex2.approx.ftz.f32 %f254,%f255;
	// inline asm
	add.f32 	%f307, %f302, 0f00000000;
	ex2.approx.f32 	%f308, %f307;
	mul.f32 	%f309, %f254, %f308;
	setp.lt.f32	%p35, %f300, 0fC2D20000;
	selp.f32	%f310, 0f00000000, %f309, %p35;
	setp.gt.f32	%p36, %f300, 0f42D20000;
	selp.f32	%f425, 0f7F800000, %f310, %p36;
	setp.eq.f32	%p37, %f425, 0f7F800000;
	@%p37 bra 	BB5_18;

	fma.rn.f32 	%f425, %f425, %f32, %f425;

BB5_18:
	add.f32 	%f311, %f10, %f31;
	mov.b32 	 %r183, %f311;
	setp.lt.s32	%p38, %r183, 2139095040;
	@%p38 bra 	BB5_25;

	setp.gtu.f32	%p39, %f10, 0f7F800000;
	setp.gtu.f32	%p40, %f31, 0f7F800000;
	or.pred  	%p41, %p39, %p40;
	@%p41 bra 	BB5_24;
	bra.uni 	BB5_20;

BB5_24:
	add.f32 	%f425, %f30, 0f40000000;
	bra.uni 	BB5_25;

BB5_20:
	setp.eq.f32	%p42, %f31, 0f7F800000;
	@%p42 bra 	BB5_23;
	bra.uni 	BB5_21;

BB5_23:
	setp.gt.f32	%p45, %f10, 0f3F800000;
	selp.b32	%r184, 2139095040, 0, %p45;
	xor.b32  	%r185, %r184, 2139095040;
	setp.lt.f32	%p46, %f30, 0f00000000;
	selp.b32	%r186, %r185, %r184, %p46;
	mov.b32 	 %f425, %r186;
	bra.uni 	BB5_25;

BB5_21:
	setp.neu.f32	%p43, %f10, 0f7F800000;
	@%p43 bra 	BB5_25;

	setp.ltu.f32	%p44, %f30, 0f00000000;
	selp.f32	%f425, 0f00000000, 0f7F800000, %p44;

BB5_25:
	ld.param.f32 	%f421, [buildFrequencyDataKernel_param_8];
	setp.eq.f32	%p47, %f30, 0f00000000;
	selp.f32	%f312, 0f3F800000, %f425, %p47;
	div.rn.f32 	%f40, %f4, %f312;
	add.f32 	%f313, %f421, 0f3DCCCCCD;
	fma.rn.f32 	%f426, %f1, %f25, %f313;
	abs.f32 	%f314, %f426;
	setp.neu.f32	%p48, %f314, 0f7F800000;
	@%p48 bra 	BB5_27;

	mul.rn.f32 	%f426, %f426, %f204;

BB5_27:
	mul.f32 	%f316, %f426, 0f3F22F983;
	cvt.rni.s32.f32	%r394, %f316;
	cvt.rn.f32.s32	%f317, %r394;
	neg.f32 	%f318, %f317;
	mov.f32 	%f319, 0f3FC90FDA;
	fma.rn.f32 	%f320, %f318, %f319, %f426;
	mov.f32 	%f321, 0f33A22168;
	fma.rn.f32 	%f322, %f318, %f321, %f320;
	mov.f32 	%f323, 0f27C234C5;
	fma.rn.f32 	%f427, %f318, %f323, %f322;
	abs.f32 	%f324, %f426;
	add.u64 	%rd39, %SP, 0;
	cvta.to.local.u64 	%rd72, %rd39;
	add.s64 	%rd1, %rd72, 24;
	setp.leu.f32	%p49, %f324, 0f47CE4780;
	@%p49 bra 	BB5_37;

	mov.b32 	 %r4, %f426;
	shr.u32 	%r5, %r4, 23;
	bfe.u32 	%r189, %r4, 23, 8;
	add.s32 	%r190, %r189, -128;
	shl.b32 	%r191, %r4, 8;
	or.b32  	%r6, %r191, -2147483648;
	shr.u32 	%r7, %r190, 5;
	cvta.to.local.u64 	%rd66, %rd39;
	mov.u32 	%r386, 0;
	mov.u64 	%rd65, __cudart_i2opi_f;
	mov.u32 	%r385, -6;

BB5_29:
	.pragma "nounroll";
	ld.const.u32 	%r194, [%rd65];
	// inline asm
	{
	mad.lo.cc.u32   %r192, %r194, %r6, %r386;
	madc.hi.u32     %r386, %r194, %r6,  0;
	}
	// inline asm
	st.local.u32 	[%rd66], %r192;
	add.s64 	%rd66, %rd66, 4;
	add.s64 	%rd65, %rd65, 4;
	add.s32 	%r385, %r385, 1;
	setp.ne.s32	%p50, %r385, 0;
	@%p50 bra 	BB5_29;

	and.b32  	%r12, %r4, -2147483648;
	st.local.u32 	[%rd1], %r386;
	mov.u32 	%r197, 6;
	sub.s32 	%r198, %r197, %r7;
	cvta.to.local.u64 	%rd44, %rd39;
	mul.wide.s32 	%rd45, %r198, 4;
	add.s64 	%rd7, %rd44, %rd45;
	ld.local.u32 	%r387, [%rd7];
	ld.local.u32 	%r388, [%rd7+-4];
	and.b32  	%r15, %r5, 31;
	setp.eq.s32	%p51, %r15, 0;
	@%p51 bra 	BB5_32;

	mov.u32 	%r199, 32;
	sub.s32 	%r200, %r199, %r15;
	shr.u32 	%r201, %r388, %r200;
	shl.b32 	%r202, %r387, %r15;
	add.s32 	%r387, %r201, %r202;
	ld.local.u32 	%r203, [%rd7+-8];
	shr.u32 	%r204, %r203, %r200;
	shl.b32 	%r205, %r388, %r15;
	add.s32 	%r388, %r204, %r205;

BB5_32:
	shr.u32 	%r206, %r388, 30;
	shl.b32 	%r207, %r387, 2;
	add.s32 	%r389, %r206, %r207;
	shl.b32 	%r21, %r388, 2;
	shr.u32 	%r208, %r389, 31;
	shr.u32 	%r209, %r387, 30;
	add.s32 	%r22, %r208, %r209;
	setp.eq.s32	%p52, %r208, 0;
	mov.u32 	%r390, %r12;
	mov.u32 	%r391, %r21;
	@%p52 bra 	BB5_34;

	not.b32 	%r210, %r389;
	neg.s32 	%r23, %r21;
	setp.eq.s32	%p53, %r21, 0;
	selp.u32	%r211, 1, 0, %p53;
	add.s32 	%r389, %r211, %r210;
	xor.b32  	%r25, %r12, -2147483648;
	mov.u32 	%r390, %r25;
	mov.u32 	%r391, %r23;

BB5_34:
	mov.u32 	%r27, %r390;
	neg.s32 	%r212, %r22;
	setp.eq.s32	%p54, %r12, 0;
	selp.b32	%r394, %r22, %r212, %p54;
	clz.b32 	%r393, %r389;
	setp.eq.s32	%p55, %r393, 0;
	shl.b32 	%r213, %r389, %r393;
	mov.u32 	%r214, 32;
	sub.s32 	%r215, %r214, %r393;
	shr.u32 	%r216, %r391, %r215;
	add.s32 	%r217, %r216, %r213;
	selp.b32	%r31, %r389, %r217, %p55;
	mov.u32 	%r218, -921707870;
	mul.hi.u32 	%r392, %r31, %r218;
	setp.lt.s32	%p56, %r392, 1;
	@%p56 bra 	BB5_36;

	mul.lo.s32 	%r219, %r31, -921707870;
	shr.u32 	%r220, %r219, 31;
	shl.b32 	%r221, %r392, 1;
	add.s32 	%r392, %r220, %r221;
	add.s32 	%r393, %r393, 1;

BB5_36:
	mov.u32 	%r222, 126;
	sub.s32 	%r223, %r222, %r393;
	shl.b32 	%r224, %r223, 23;
	add.s32 	%r225, %r392, 1;
	shr.u32 	%r226, %r225, 7;
	add.s32 	%r227, %r226, 1;
	shr.u32 	%r228, %r227, 1;
	add.s32 	%r229, %r228, %r224;
	or.b32  	%r230, %r229, %r27;
	mov.b32 	 %f427, %r230;

BB5_37:
	mul.rn.f32 	%f47, %f427, %f427;
	and.b32  	%r38, %r394, 1;
	setp.eq.s32	%p57, %r38, 0;
	@%p57 bra 	BB5_39;

	mov.f32 	%f325, 0fBAB6061A;
	mov.f32 	%f326, 0f37CCF5CE;
	fma.rn.f32 	%f428, %f326, %f47, %f325;
	bra.uni 	BB5_40;

BB5_39:
	mov.f32 	%f327, 0f3C08839E;
	mov.f32 	%f328, 0fB94CA1F9;
	fma.rn.f32 	%f428, %f328, %f47, %f327;

BB5_40:
	@%p57 bra 	BB5_42;

	mov.f32 	%f329, 0f3D2AAAA5;
	fma.rn.f32 	%f330, %f428, %f47, %f329;
	mov.f32 	%f331, 0fBF000000;
	fma.rn.f32 	%f429, %f330, %f47, %f331;
	bra.uni 	BB5_43;

BB5_42:
	mov.f32 	%f332, 0fBE2AAAA3;
	fma.rn.f32 	%f333, %f428, %f47, %f332;
	fma.rn.f32 	%f429, %f333, %f47, %f204;

BB5_43:
	fma.rn.f32 	%f430, %f429, %f427, %f427;
	@%p57 bra 	BB5_45;

	mov.f32 	%f335, 0f3F800000;
	fma.rn.f32 	%f430, %f429, %f47, %f335;

BB5_45:
	and.b32  	%r231, %r394, 2;
	setp.eq.s32	%p60, %r231, 0;
	@%p60 bra 	BB5_47;

	mov.f32 	%f337, 0fBF800000;
	fma.rn.f32 	%f430, %f430, %f337, %f204;

BB5_47:
	fma.rn.f32 	%f431, %f2, %f25, %f313;
	abs.f32 	%f339, %f431;
	setp.neu.f32	%p61, %f339, 0f7F800000;
	@%p61 bra 	BB5_49;

	mul.rn.f32 	%f431, %f431, %f204;

BB5_49:
	mul.f32 	%f341, %f431, 0f3F22F983;
	cvt.rni.s32.f32	%r404, %f341;
	cvt.rn.f32.s32	%f342, %r404;
	neg.f32 	%f343, %f342;
	fma.rn.f32 	%f345, %f343, %f319, %f431;
	fma.rn.f32 	%f347, %f343, %f321, %f345;
	fma.rn.f32 	%f432, %f343, %f323, %f347;
	abs.f32 	%f349, %f431;
	setp.leu.f32	%p62, %f349, 0f47CE4780;
	@%p62 bra 	BB5_59;

	mov.b32 	 %r40, %f431;
	shr.u32 	%r41, %r40, 23;
	bfe.u32 	%r234, %r40, 23, 8;
	add.s32 	%r235, %r234, -128;
	shl.b32 	%r236, %r40, 8;
	or.b32  	%r42, %r236, -2147483648;
	shr.u32 	%r43, %r235, 5;
	cvta.to.local.u64 	%rd68, %rd39;
	mov.u32 	%r396, 0;
	mov.u64 	%rd67, __cudart_i2opi_f;
	mov.u32 	%r395, -6;

BB5_51:
	.pragma "nounroll";
	ld.const.u32 	%r239, [%rd67];
	// inline asm
	{
	mad.lo.cc.u32   %r237, %r239, %r42, %r396;
	madc.hi.u32     %r396, %r239, %r42,  0;
	}
	// inline asm
	st.local.u32 	[%rd68], %r237;
	add.s64 	%rd68, %rd68, 4;
	add.s64 	%rd67, %rd67, 4;
	add.s32 	%r395, %r395, 1;
	setp.ne.s32	%p63, %r395, 0;
	@%p63 bra 	BB5_51;

	and.b32  	%r48, %r40, -2147483648;
	st.local.u32 	[%rd1], %r396;
	mov.u32 	%r242, 6;
	sub.s32 	%r243, %r242, %r43;
	cvta.to.local.u64 	%rd49, %rd39;
	mul.wide.s32 	%rd50, %r243, 4;
	add.s64 	%rd13, %rd49, %rd50;
	ld.local.u32 	%r397, [%rd13];
	ld.local.u32 	%r398, [%rd13+-4];
	and.b32  	%r51, %r41, 31;
	setp.eq.s32	%p64, %r51, 0;
	@%p64 bra 	BB5_54;

	mov.u32 	%r244, 32;
	sub.s32 	%r245, %r244, %r51;
	shr.u32 	%r246, %r398, %r245;
	shl.b32 	%r247, %r397, %r51;
	add.s32 	%r397, %r246, %r247;
	ld.local.u32 	%r248, [%rd13+-8];
	shr.u32 	%r249, %r248, %r245;
	shl.b32 	%r250, %r398, %r51;
	add.s32 	%r398, %r249, %r250;

BB5_54:
	shr.u32 	%r251, %r398, 30;
	shl.b32 	%r252, %r397, 2;
	add.s32 	%r399, %r251, %r252;
	shl.b32 	%r57, %r398, 2;
	shr.u32 	%r253, %r399, 31;
	shr.u32 	%r254, %r397, 30;
	add.s32 	%r58, %r253, %r254;
	setp.eq.s32	%p65, %r253, 0;
	mov.u32 	%r400, %r48;
	mov.u32 	%r401, %r57;
	@%p65 bra 	BB5_56;

	not.b32 	%r255, %r399;
	neg.s32 	%r59, %r57;
	setp.eq.s32	%p66, %r57, 0;
	selp.u32	%r256, 1, 0, %p66;
	add.s32 	%r399, %r256, %r255;
	xor.b32  	%r61, %r48, -2147483648;
	mov.u32 	%r400, %r61;
	mov.u32 	%r401, %r59;

BB5_56:
	mov.u32 	%r63, %r400;
	neg.s32 	%r257, %r58;
	setp.eq.s32	%p67, %r48, 0;
	selp.b32	%r404, %r58, %r257, %p67;
	clz.b32 	%r403, %r399;
	setp.eq.s32	%p68, %r403, 0;
	shl.b32 	%r258, %r399, %r403;
	mov.u32 	%r259, 32;
	sub.s32 	%r260, %r259, %r403;
	shr.u32 	%r261, %r401, %r260;
	add.s32 	%r262, %r261, %r258;
	selp.b32	%r67, %r399, %r262, %p68;
	mov.u32 	%r263, -921707870;
	mul.hi.u32 	%r402, %r67, %r263;
	setp.lt.s32	%p69, %r402, 1;
	@%p69 bra 	BB5_58;

	mul.lo.s32 	%r264, %r67, -921707870;
	shr.u32 	%r265, %r264, 31;
	shl.b32 	%r266, %r402, 1;
	add.s32 	%r402, %r265, %r266;
	add.s32 	%r403, %r403, 1;

BB5_58:
	mov.u32 	%r267, 126;
	sub.s32 	%r268, %r267, %r403;
	shl.b32 	%r269, %r268, 23;
	add.s32 	%r270, %r402, 1;
	shr.u32 	%r271, %r270, 7;
	add.s32 	%r272, %r271, 1;
	shr.u32 	%r273, %r272, 1;
	add.s32 	%r274, %r273, %r269;
	or.b32  	%r275, %r274, %r63;
	mov.b32 	 %f432, %r275;

BB5_59:
	mul.rn.f32 	%f65, %f432, %f432;
	add.s32 	%r74, %r404, 1;
	and.b32  	%r75, %r74, 1;
	setp.eq.s32	%p70, %r75, 0;
	@%p70 bra 	BB5_61;

	mov.f32 	%f350, 0fBAB6061A;
	mov.f32 	%f351, 0f37CCF5CE;
	fma.rn.f32 	%f433, %f351, %f65, %f350;
	bra.uni 	BB5_62;

BB5_61:
	mov.f32 	%f352, 0f3C08839E;
	mov.f32 	%f353, 0fB94CA1F9;
	fma.rn.f32 	%f433, %f353, %f65, %f352;

BB5_62:
	@%p70 bra 	BB5_64;

	mov.f32 	%f354, 0f3D2AAAA5;
	fma.rn.f32 	%f355, %f433, %f65, %f354;
	mov.f32 	%f356, 0fBF000000;
	fma.rn.f32 	%f434, %f355, %f65, %f356;
	bra.uni 	BB5_65;

BB5_64:
	mov.f32 	%f357, 0fBE2AAAA3;
	fma.rn.f32 	%f358, %f433, %f65, %f357;
	fma.rn.f32 	%f434, %f358, %f65, %f204;

BB5_65:
	fma.rn.f32 	%f435, %f434, %f432, %f432;
	@%p70 bra 	BB5_67;

	mov.f32 	%f360, 0f3F800000;
	fma.rn.f32 	%f435, %f434, %f65, %f360;

BB5_67:
	and.b32  	%r276, %r74, 2;
	setp.eq.s32	%p73, %r276, 0;
	@%p73 bra 	BB5_69;

	mov.f32 	%f362, 0fBF800000;
	fma.rn.f32 	%f435, %f435, %f362, %f204;

BB5_69:
	fma.rn.f32 	%f436, %f2, %f40, %f313;
	abs.f32 	%f364, %f436;
	setp.neu.f32	%p74, %f364, 0f7F800000;
	@%p74 bra 	BB5_71;

	mul.rn.f32 	%f436, %f436, %f204;

BB5_71:
	mul.f32 	%f80, %f430, %f435;
	mul.f32 	%f366, %f436, 0f3F22F983;
	cvt.rni.s32.f32	%r414, %f366;
	cvt.rn.f32.s32	%f367, %r414;
	neg.f32 	%f368, %f367;
	fma.rn.f32 	%f370, %f368, %f319, %f436;
	fma.rn.f32 	%f372, %f368, %f321, %f370;
	fma.rn.f32 	%f437, %f368, %f323, %f372;
	abs.f32 	%f374, %f436;
	setp.leu.f32	%p75, %f374, 0f47CE4780;
	@%p75 bra 	BB5_81;

	mov.b32 	 %r77, %f436;
	shr.u32 	%r78, %r77, 23;
	bfe.u32 	%r279, %r77, 23, 8;
	add.s32 	%r280, %r279, -128;
	shl.b32 	%r281, %r77, 8;
	or.b32  	%r79, %r281, -2147483648;
	shr.u32 	%r80, %r280, 5;
	cvta.to.local.u64 	%rd70, %rd39;
	mov.u32 	%r406, 0;
	mov.u64 	%rd69, __cudart_i2opi_f;
	mov.u32 	%r405, -6;

BB5_73:
	.pragma "nounroll";
	ld.const.u32 	%r284, [%rd69];
	// inline asm
	{
	mad.lo.cc.u32   %r282, %r284, %r79, %r406;
	madc.hi.u32     %r406, %r284, %r79,  0;
	}
	// inline asm
	st.local.u32 	[%rd70], %r282;
	add.s64 	%rd70, %rd70, 4;
	add.s64 	%rd69, %rd69, 4;
	add.s32 	%r405, %r405, 1;
	setp.ne.s32	%p76, %r405, 0;
	@%p76 bra 	BB5_73;

	and.b32  	%r85, %r77, -2147483648;
	st.local.u32 	[%rd1], %r406;
	mov.u32 	%r287, 6;
	sub.s32 	%r288, %r287, %r80;
	cvta.to.local.u64 	%rd54, %rd39;
	mul.wide.s32 	%rd55, %r288, 4;
	add.s64 	%rd19, %rd54, %rd55;
	ld.local.u32 	%r407, [%rd19];
	ld.local.u32 	%r408, [%rd19+-4];
	and.b32  	%r88, %r78, 31;
	setp.eq.s32	%p77, %r88, 0;
	@%p77 bra 	BB5_76;

	mov.u32 	%r289, 32;
	sub.s32 	%r290, %r289, %r88;
	shr.u32 	%r291, %r408, %r290;
	shl.b32 	%r292, %r407, %r88;
	add.s32 	%r407, %r291, %r292;
	ld.local.u32 	%r293, [%rd19+-8];
	shr.u32 	%r294, %r293, %r290;
	shl.b32 	%r295, %r408, %r88;
	add.s32 	%r408, %r294, %r295;

BB5_76:
	shr.u32 	%r296, %r408, 30;
	shl.b32 	%r297, %r407, 2;
	add.s32 	%r409, %r296, %r297;
	shl.b32 	%r94, %r408, 2;
	shr.u32 	%r298, %r409, 31;
	shr.u32 	%r299, %r407, 30;
	add.s32 	%r95, %r298, %r299;
	setp.eq.s32	%p78, %r298, 0;
	mov.u32 	%r410, %r85;
	mov.u32 	%r411, %r94;
	@%p78 bra 	BB5_78;

	not.b32 	%r300, %r409;
	neg.s32 	%r96, %r94;
	setp.eq.s32	%p79, %r94, 0;
	selp.u32	%r301, 1, 0, %p79;
	add.s32 	%r409, %r301, %r300;
	xor.b32  	%r98, %r85, -2147483648;
	mov.u32 	%r410, %r98;
	mov.u32 	%r411, %r96;

BB5_78:
	mov.u32 	%r100, %r410;
	neg.s32 	%r302, %r95;
	setp.eq.s32	%p80, %r85, 0;
	selp.b32	%r414, %r95, %r302, %p80;
	clz.b32 	%r413, %r409;
	setp.eq.s32	%p81, %r413, 0;
	shl.b32 	%r303, %r409, %r413;
	mov.u32 	%r304, 32;
	sub.s32 	%r305, %r304, %r413;
	shr.u32 	%r306, %r411, %r305;
	add.s32 	%r307, %r306, %r303;
	selp.b32	%r104, %r409, %r307, %p81;
	mov.u32 	%r308, -921707870;
	mul.hi.u32 	%r412, %r104, %r308;
	setp.lt.s32	%p82, %r412, 1;
	@%p82 bra 	BB5_80;

	mul.lo.s32 	%r309, %r104, -921707870;
	shr.u32 	%r310, %r309, 31;
	shl.b32 	%r311, %r412, 1;
	add.s32 	%r412, %r310, %r311;
	add.s32 	%r413, %r413, 1;

BB5_80:
	mov.u32 	%r312, 126;
	sub.s32 	%r313, %r312, %r413;
	shl.b32 	%r314, %r313, 23;
	add.s32 	%r315, %r412, 1;
	shr.u32 	%r316, %r315, 7;
	add.s32 	%r317, %r316, 1;
	shr.u32 	%r318, %r317, 1;
	add.s32 	%r319, %r318, %r314;
	or.b32  	%r320, %r319, %r100;
	mov.b32 	 %f437, %r320;

BB5_81:
	mul.rn.f32 	%f84, %f437, %f437;
	and.b32  	%r111, %r414, 1;
	setp.eq.s32	%p83, %r111, 0;
	@%p83 bra 	BB5_83;

	mov.f32 	%f375, 0fBAB6061A;
	mov.f32 	%f376, 0f37CCF5CE;
	fma.rn.f32 	%f438, %f376, %f84, %f375;
	bra.uni 	BB5_84;

BB5_83:
	mov.f32 	%f377, 0f3C08839E;
	mov.f32 	%f378, 0fB94CA1F9;
	fma.rn.f32 	%f438, %f378, %f84, %f377;

BB5_84:
	@%p83 bra 	BB5_86;

	mov.f32 	%f379, 0f3D2AAAA5;
	fma.rn.f32 	%f380, %f438, %f84, %f379;
	mov.f32 	%f381, 0fBF000000;
	fma.rn.f32 	%f439, %f380, %f84, %f381;
	bra.uni 	BB5_87;

BB5_86:
	mov.f32 	%f382, 0fBE2AAAA3;
	fma.rn.f32 	%f383, %f438, %f84, %f382;
	fma.rn.f32 	%f439, %f383, %f84, %f204;

BB5_87:
	fma.rn.f32 	%f440, %f439, %f437, %f437;
	@%p83 bra 	BB5_89;

	mov.f32 	%f385, 0f3F800000;
	fma.rn.f32 	%f440, %f439, %f84, %f385;

BB5_89:
	and.b32  	%r321, %r414, 2;
	setp.eq.s32	%p86, %r321, 0;
	@%p86 bra 	BB5_91;

	mov.f32 	%f387, 0fBF800000;
	fma.rn.f32 	%f440, %f440, %f387, %f204;

BB5_91:
	fma.rn.f32 	%f441, %f1, %f40, %f313;
	abs.f32 	%f389, %f441;
	setp.neu.f32	%p87, %f389, 0f7F800000;
	@%p87 bra 	BB5_93;

	mul.rn.f32 	%f441, %f441, %f204;

BB5_93:
	mul.f32 	%f391, %f441, 0f3F22F983;
	cvt.rni.s32.f32	%r424, %f391;
	cvt.rn.f32.s32	%f392, %r424;
	neg.f32 	%f393, %f392;
	fma.rn.f32 	%f395, %f393, %f319, %f441;
	fma.rn.f32 	%f397, %f393, %f321, %f395;
	fma.rn.f32 	%f442, %f393, %f323, %f397;
	abs.f32 	%f399, %f441;
	setp.leu.f32	%p88, %f399, 0f47CE4780;
	@%p88 bra 	BB5_103;

	mov.b32 	 %r113, %f441;
	shr.u32 	%r114, %r113, 23;
	bfe.u32 	%r324, %r113, 23, 8;
	add.s32 	%r325, %r324, -128;
	shl.b32 	%r326, %r113, 8;
	or.b32  	%r115, %r326, -2147483648;
	shr.u32 	%r116, %r325, 5;
	mov.u32 	%r416, 0;
	mov.u64 	%rd71, __cudart_i2opi_f;
	mov.u32 	%r415, -6;

BB5_95:
	.pragma "nounroll";
	ld.const.u32 	%r329, [%rd71];
	// inline asm
	{
	mad.lo.cc.u32   %r327, %r329, %r115, %r416;
	madc.hi.u32     %r416, %r329, %r115,  0;
	}
	// inline asm
	st.local.u32 	[%rd72], %r327;
	add.s64 	%rd72, %rd72, 4;
	add.s64 	%rd71, %rd71, 4;
	add.s32 	%r415, %r415, 1;
	setp.ne.s32	%p89, %r415, 0;
	@%p89 bra 	BB5_95;

	and.b32  	%r121, %r113, -2147483648;
	st.local.u32 	[%rd1], %r416;
	mov.u32 	%r332, 6;
	sub.s32 	%r333, %r332, %r116;
	cvta.to.local.u64 	%rd59, %rd39;
	mul.wide.s32 	%rd60, %r333, 4;
	add.s64 	%rd25, %rd59, %rd60;
	ld.local.u32 	%r417, [%rd25];
	ld.local.u32 	%r418, [%rd25+-4];
	and.b32  	%r124, %r114, 31;
	setp.eq.s32	%p90, %r124, 0;
	@%p90 bra 	BB5_98;

	mov.u32 	%r334, 32;
	sub.s32 	%r335, %r334, %r124;
	shr.u32 	%r336, %r418, %r335;
	shl.b32 	%r337, %r417, %r124;
	add.s32 	%r417, %r336, %r337;
	ld.local.u32 	%r338, [%rd25+-8];
	shr.u32 	%r339, %r338, %r335;
	shl.b32 	%r340, %r418, %r124;
	add.s32 	%r418, %r339, %r340;

BB5_98:
	shr.u32 	%r341, %r418, 30;
	shl.b32 	%r342, %r417, 2;
	add.s32 	%r419, %r341, %r342;
	shl.b32 	%r130, %r418, 2;
	shr.u32 	%r343, %r419, 31;
	shr.u32 	%r344, %r417, 30;
	add.s32 	%r131, %r343, %r344;
	setp.eq.s32	%p91, %r343, 0;
	mov.u32 	%r420, %r121;
	mov.u32 	%r421, %r130;
	@%p91 bra 	BB5_100;

	not.b32 	%r345, %r419;
	neg.s32 	%r132, %r130;
	setp.eq.s32	%p92, %r130, 0;
	selp.u32	%r346, 1, 0, %p92;
	add.s32 	%r419, %r346, %r345;
	xor.b32  	%r134, %r121, -2147483648;
	mov.u32 	%r420, %r134;
	mov.u32 	%r421, %r132;

BB5_100:
	mov.u32 	%r136, %r420;
	neg.s32 	%r347, %r131;
	setp.eq.s32	%p93, %r121, 0;
	selp.b32	%r424, %r131, %r347, %p93;
	clz.b32 	%r423, %r419;
	setp.eq.s32	%p94, %r423, 0;
	shl.b32 	%r348, %r419, %r423;
	mov.u32 	%r349, 32;
	sub.s32 	%r350, %r349, %r423;
	shr.u32 	%r351, %r421, %r350;
	add.s32 	%r352, %r351, %r348;
	selp.b32	%r140, %r419, %r352, %p94;
	mov.u32 	%r353, -921707870;
	mul.hi.u32 	%r422, %r140, %r353;
	setp.lt.s32	%p95, %r422, 1;
	@%p95 bra 	BB5_102;

	mul.lo.s32 	%r354, %r140, -921707870;
	shr.u32 	%r355, %r354, 31;
	shl.b32 	%r356, %r422, 1;
	add.s32 	%r422, %r355, %r356;
	add.s32 	%r423, %r423, 1;

BB5_102:
	mov.u32 	%r357, 126;
	sub.s32 	%r358, %r357, %r423;
	shl.b32 	%r359, %r358, 23;
	add.s32 	%r360, %r422, 1;
	shr.u32 	%r361, %r360, 7;
	add.s32 	%r362, %r361, 1;
	shr.u32 	%r363, %r362, 1;
	add.s32 	%r364, %r363, %r359;
	or.b32  	%r365, %r364, %r136;
	mov.b32 	 %f442, %r365;

BB5_103:
	mul.rn.f32 	%f102, %f442, %f442;
	add.s32 	%r147, %r424, 1;
	and.b32  	%r148, %r147, 1;
	setp.eq.s32	%p96, %r148, 0;
	@%p96 bra 	BB5_105;

	mov.f32 	%f400, 0fBAB6061A;
	mov.f32 	%f401, 0f37CCF5CE;
	fma.rn.f32 	%f443, %f401, %f102, %f400;
	bra.uni 	BB5_106;

BB5_105:
	mov.f32 	%f402, 0f3C08839E;
	mov.f32 	%f403, 0fB94CA1F9;
	fma.rn.f32 	%f443, %f403, %f102, %f402;

BB5_106:
	@%p96 bra 	BB5_108;

	mov.f32 	%f404, 0f3D2AAAA5;
	fma.rn.f32 	%f405, %f443, %f102, %f404;
	mov.f32 	%f406, 0fBF000000;
	fma.rn.f32 	%f444, %f405, %f102, %f406;
	bra.uni 	BB5_109;

BB5_108:
	mov.f32 	%f407, 0fBE2AAAA3;
	fma.rn.f32 	%f408, %f443, %f102, %f407;
	fma.rn.f32 	%f444, %f408, %f102, %f204;

BB5_109:
	fma.rn.f32 	%f445, %f444, %f442, %f442;
	@%p96 bra 	BB5_111;

	mov.f32 	%f410, 0f3F800000;
	fma.rn.f32 	%f445, %f444, %f102, %f410;

BB5_111:
	and.b32  	%r366, %r147, 2;
	setp.eq.s32	%p99, %r366, 0;
	@%p99 bra 	BB5_113;

	mov.f32 	%f412, 0fBF800000;
	fma.rn.f32 	%f445, %f445, %f412, %f204;

BB5_113:
	ld.param.u64 	%rd64, [buildFrequencyDataKernel_param_0];
	ld.param.u32 	%r384, [buildFrequencyDataKernel_param_4];
	mov.u32 	%r383, %tid.y;
	mov.u32 	%r382, %ctaid.y;
	mov.u32 	%r381, %ntid.y;
	mad.lo.s32 	%r380, %r381, %r382, %r383;
	mov.u32 	%r379, %tid.x;
	mov.u32 	%r378, %ctaid.x;
	mov.u32 	%r377, %ntid.x;
	mad.lo.s32 	%r376, %r377, %r378, %r379;
	mad.lo.s32 	%r375, %r380, %r384, %r376;
	mul.f32 	%f413, %f440, %f445;
	cvta.to.global.u64 	%rd61, %rd64;
	mul.wide.u32 	%rd62, %r375, 8;
	add.s64 	%rd63, %rd61, %rd62;
	mul.f32 	%f414, %f413, 0f3DCCCCCD;
	mul.f32 	%f415, %f80, 0f3DCCCCCD;
	st.global.v2.f32 	[%rd63], {%f415, %f414};

BB5_114:
	ret;
}

	// .globl	generateSpectrumKernel
.visible .entry generateSpectrumKernel(
	.param .u64 generateSpectrumKernel_param_0,
	.param .u64 generateSpectrumKernel_param_1,
	.param .u64 generateSpectrumKernel_param_2,
	.param .u32 generateSpectrumKernel_param_3,
	.param .u32 generateSpectrumKernel_param_4,
	.param .u32 generateSpectrumKernel_param_5,
	.param .f32 generateSpectrumKernel_param_6,
	.param .f32 generateSpectrumKernel_param_7,
	.param .f32 generateSpectrumKernel_param_8
)
{
	.local .align 4 .b8 	__local_depot6[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<56>;
	.reg .f32 	%f<233>;
	.reg .b32 	%r<392>;
	.reg .b64 	%rd<67>;


	mov.u64 	%rd66, __local_depot6;
	cvta.local.u64 	%SP, %rd66;
	ld.param.u64 	%rd25, [generateSpectrumKernel_param_0];
	ld.param.u64 	%rd26, [generateSpectrumKernel_param_1];
	ld.param.u64 	%rd27, [generateSpectrumKernel_param_2];
	ld.param.u32 	%r147, [generateSpectrumKernel_param_3];
	ld.param.u32 	%r148, [generateSpectrumKernel_param_4];
	ld.param.u32 	%r149, [generateSpectrumKernel_param_5];
	ld.param.f32 	%f79, [generateSpectrumKernel_param_6];
	ld.param.f32 	%f80, [generateSpectrumKernel_param_7];
	ld.param.f32 	%f81, [generateSpectrumKernel_param_8];
	mov.u32 	%r150, %ntid.x;
	mov.u32 	%r151, %ctaid.x;
	mov.u32 	%r152, %tid.x;
	mad.lo.s32 	%r1, %r150, %r151, %r152;
	mov.u32 	%r153, %ntid.y;
	mov.u32 	%r154, %ctaid.y;
	mov.u32 	%r155, %tid.y;
	mad.lo.s32 	%r2, %r153, %r154, %r155;
	setp.lt.u32	%p1, %r1, %r148;
	setp.lt.u32	%p2, %r2, %r149;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB6_90;
	bra.uni 	BB6_1;

BB6_1:
	cvta.to.global.u64 	%rd28, %rd25;
	mad.lo.s32 	%r156, %r2, %r147, %r1;
	sub.s32 	%r157, %r149, %r2;
	sub.s32 	%r158, %r148, %r1;
	mad.lo.s32 	%r159, %r157, %r147, %r158;
	neg.s32 	%r160, %r148;
	cvt.rn.f32.s32	%f82, %r160;
	cvt.rn.f32.u32	%f83, %r1;
	fma.rn.f32 	%f84, %f82, 0f3F000000, %f83;
	mov.f32 	%f85, 0f40C90FDB;
	div.rn.f32 	%f86, %f85, %f81;
	mul.f32 	%f87, %f84, %f86;
	neg.s32 	%r161, %r149;
	cvt.rn.f32.s32	%f88, %r161;
	cvt.rn.f32.u32	%f89, %r2;
	fma.rn.f32 	%f90, %f88, 0f3F000000, %f89;
	mul.f32 	%f91, %f86, %f90;
	mul.f32 	%f92, %f91, %f91;
	fma.rn.f32 	%f93, %f87, %f87, %f92;
	sqrt.rn.f32 	%f94, %f93;
	mul.f32 	%f95, %f94, 0f411CF5C3;
	sqrt.rn.f32 	%f96, %f95;
	mul.wide.u32 	%rd29, %r156, 8;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.v2.f32 	{%f97, %f98}, [%rd30];
	mul.wide.u32 	%rd31, %r159, 8;
	add.s64 	%rd32, %rd28, %rd31;
	ld.global.v2.f32 	{%f99, %f100}, [%rd32];
	mul.f32 	%f5, %f96, %f79;
	abs.f32 	%f6, %f5;
	setp.neu.f32	%p4, %f6, 0f7F800000;
	mov.f32 	%f217, %f5;
	@%p4 bra 	BB6_3;

	mov.f32 	%f101, 0f00000000;
	mul.rn.f32 	%f7, %f5, %f101;
	mov.f32 	%f217, %f7;

BB6_3:
	mov.f32 	%f8, %f217;
	mul.f32 	%f102, %f8, 0f3F22F983;
	cvt.rni.s32.f32	%r361, %f102;
	cvt.rn.f32.s32	%f103, %r361;
	neg.f32 	%f104, %f103;
	mov.f32 	%f105, 0f3FC90FDA;
	fma.rn.f32 	%f106, %f104, %f105, %f8;
	mov.f32 	%f107, 0f33A22168;
	fma.rn.f32 	%f108, %f104, %f107, %f106;
	mov.f32 	%f109, 0f27C234C5;
	fma.rn.f32 	%f211, %f104, %f109, %f108;
	abs.f32 	%f110, %f8;
	setp.leu.f32	%p5, %f110, 0f47CE4780;
	@%p5 bra 	BB6_13;

	mov.b32 	 %r4, %f8;
	shl.b32 	%r164, %r4, 8;
	or.b32  	%r5, %r164, -2147483648;
	add.u64 	%rd34, %SP, 0;
	cvta.to.local.u64 	%rd59, %rd34;
	mov.u32 	%r353, 0;
	mov.u64 	%rd58, __cudart_i2opi_f;
	mov.u32 	%r352, -6;

BB6_5:
	.pragma "nounroll";
	ld.const.u32 	%r167, [%rd58];
	// inline asm
	{
	mad.lo.cc.u32   %r165, %r167, %r5, %r353;
	madc.hi.u32     %r353, %r167, %r5,  0;
	}
	// inline asm
	st.local.u32 	[%rd59], %r165;
	add.s64 	%rd59, %rd59, 4;
	add.s64 	%rd58, %rd58, 4;
	add.s32 	%r352, %r352, 1;
	setp.ne.s32	%p6, %r352, 0;
	@%p6 bra 	BB6_5;

	and.b32  	%r10, %r4, -2147483648;
	bfe.u32 	%r170, %r4, 23, 8;
	add.s32 	%r171, %r170, -128;
	shr.u32 	%r172, %r171, 5;
	cvta.to.local.u64 	%rd36, %rd34;
	st.local.u32 	[%rd36+24], %r353;
	bfe.u32 	%r11, %r4, 23, 5;
	mov.u32 	%r173, 6;
	sub.s32 	%r174, %r173, %r172;
	mul.wide.s32 	%rd37, %r174, 4;
	add.s64 	%rd6, %rd36, %rd37;
	ld.local.u32 	%r354, [%rd6];
	ld.local.u32 	%r355, [%rd6+-4];
	setp.eq.s32	%p7, %r11, 0;
	@%p7 bra 	BB6_8;

	mov.u32 	%r175, 32;
	sub.s32 	%r176, %r175, %r11;
	shr.u32 	%r177, %r355, %r176;
	shl.b32 	%r178, %r354, %r11;
	add.s32 	%r354, %r177, %r178;
	ld.local.u32 	%r179, [%rd6+-8];
	shr.u32 	%r180, %r179, %r176;
	shl.b32 	%r181, %r355, %r11;
	add.s32 	%r355, %r180, %r181;

BB6_8:
	shr.u32 	%r182, %r355, 30;
	shl.b32 	%r183, %r354, 2;
	add.s32 	%r356, %r182, %r183;
	shl.b32 	%r19, %r355, 2;
	shr.u32 	%r184, %r356, 31;
	shr.u32 	%r185, %r354, 30;
	add.s32 	%r20, %r184, %r185;
	setp.eq.s32	%p8, %r184, 0;
	mov.u32 	%r357, %r10;
	mov.u32 	%r358, %r19;
	@%p8 bra 	BB6_10;

	not.b32 	%r186, %r356;
	neg.s32 	%r21, %r19;
	setp.eq.s32	%p9, %r19, 0;
	selp.u32	%r187, 1, 0, %p9;
	add.s32 	%r356, %r187, %r186;
	xor.b32  	%r23, %r10, -2147483648;
	mov.u32 	%r357, %r23;
	mov.u32 	%r358, %r21;

BB6_10:
	mov.u32 	%r25, %r357;
	neg.s32 	%r188, %r20;
	setp.eq.s32	%p10, %r10, 0;
	selp.b32	%r361, %r20, %r188, %p10;
	clz.b32 	%r360, %r356;
	setp.eq.s32	%p11, %r360, 0;
	shl.b32 	%r189, %r356, %r360;
	mov.u32 	%r190, 32;
	sub.s32 	%r191, %r190, %r360;
	shr.u32 	%r192, %r358, %r191;
	add.s32 	%r193, %r192, %r189;
	selp.b32	%r29, %r356, %r193, %p11;
	mov.u32 	%r194, -921707870;
	mul.hi.u32 	%r359, %r29, %r194;
	setp.lt.s32	%p12, %r359, 1;
	@%p12 bra 	BB6_12;

	mul.lo.s32 	%r195, %r29, -921707870;
	shr.u32 	%r196, %r195, 31;
	shl.b32 	%r197, %r359, 1;
	add.s32 	%r359, %r196, %r197;
	add.s32 	%r360, %r360, 1;

BB6_12:
	mov.u32 	%r198, 126;
	sub.s32 	%r199, %r198, %r360;
	shl.b32 	%r200, %r199, 23;
	add.s32 	%r201, %r359, 1;
	shr.u32 	%r202, %r201, 7;
	add.s32 	%r203, %r202, 1;
	shr.u32 	%r204, %r203, 1;
	add.s32 	%r205, %r204, %r200;
	or.b32  	%r206, %r205, %r25;
	mov.b32 	 %f211, %r206;

BB6_13:
	mul.rn.f32 	%f12, %f211, %f211;
	add.s32 	%r36, %r361, 1;
	and.b32  	%r37, %r36, 1;
	setp.eq.s32	%p13, %r37, 0;
	@%p13 bra 	BB6_15;

	mov.f32 	%f111, 0fBAB6061A;
	mov.f32 	%f112, 0f37CCF5CE;
	fma.rn.f32 	%f212, %f112, %f12, %f111;
	bra.uni 	BB6_16;

BB6_15:
	mov.f32 	%f113, 0f3C08839E;
	mov.f32 	%f114, 0fB94CA1F9;
	fma.rn.f32 	%f212, %f114, %f12, %f113;

BB6_16:
	@%p13 bra 	BB6_18;

	mov.f32 	%f115, 0f3D2AAAA5;
	fma.rn.f32 	%f116, %f212, %f12, %f115;
	mov.f32 	%f117, 0fBF000000;
	fma.rn.f32 	%f213, %f116, %f12, %f117;
	bra.uni 	BB6_19;

BB6_18:
	mov.f32 	%f118, 0fBE2AAAA3;
	fma.rn.f32 	%f119, %f212, %f12, %f118;
	mov.f32 	%f120, 0f00000000;
	fma.rn.f32 	%f213, %f119, %f12, %f120;

BB6_19:
	fma.rn.f32 	%f214, %f213, %f211, %f211;
	@%p13 bra 	BB6_21;

	mov.f32 	%f121, 0f3F800000;
	fma.rn.f32 	%f214, %f213, %f12, %f121;

BB6_21:
	and.b32  	%r207, %r36, 2;
	setp.eq.s32	%p16, %r207, 0;
	@%p16 bra 	BB6_23;

	mov.f32 	%f122, 0f00000000;
	mov.f32 	%f123, 0fBF800000;
	fma.rn.f32 	%f214, %f214, %f123, %f122;

BB6_23:
	mov.f32 	%f216, %f5;
	@%p4 bra 	BB6_25;

	mov.f32 	%f124, 0f00000000;
	mul.rn.f32 	%f216, %f5, %f124;

BB6_25:
	mul.f32 	%f125, %f216, 0f3F22F983;
	cvt.rni.s32.f32	%r371, %f125;
	cvt.rn.f32.s32	%f126, %r371;
	neg.f32 	%f127, %f126;
	fma.rn.f32 	%f129, %f127, %f105, %f216;
	fma.rn.f32 	%f131, %f127, %f107, %f129;
	fma.rn.f32 	%f218, %f127, %f109, %f131;
	abs.f32 	%f133, %f216;
	setp.leu.f32	%p18, %f133, 0f47CE4780;
	@%p18 bra 	BB6_35;

	mov.b32 	 %r39, %f216;
	shr.u32 	%r40, %r39, 23;
	bfe.u32 	%r210, %r39, 23, 8;
	add.s32 	%r211, %r210, -128;
	shl.b32 	%r212, %r39, 8;
	or.b32  	%r41, %r212, -2147483648;
	shr.u32 	%r42, %r211, 5;
	add.u64 	%rd39, %SP, 0;
	cvta.to.local.u64 	%rd61, %rd39;
	mov.u32 	%r363, 0;
	mov.u64 	%rd60, __cudart_i2opi_f;
	mov.u32 	%r362, -6;

BB6_27:
	.pragma "nounroll";
	ld.const.u32 	%r215, [%rd60];
	// inline asm
	{
	mad.lo.cc.u32   %r213, %r215, %r41, %r363;
	madc.hi.u32     %r363, %r215, %r41,  0;
	}
	// inline asm
	st.local.u32 	[%rd61], %r213;
	add.s64 	%rd61, %rd61, 4;
	add.s64 	%rd60, %rd60, 4;
	add.s32 	%r362, %r362, 1;
	setp.ne.s32	%p19, %r362, 0;
	@%p19 bra 	BB6_27;

	and.b32  	%r47, %r39, -2147483648;
	cvta.to.local.u64 	%rd41, %rd39;
	st.local.u32 	[%rd41+24], %r363;
	mov.u32 	%r218, 6;
	sub.s32 	%r219, %r218, %r42;
	mul.wide.s32 	%rd42, %r219, 4;
	add.s64 	%rd12, %rd41, %rd42;
	ld.local.u32 	%r364, [%rd12];
	ld.local.u32 	%r365, [%rd12+-4];
	and.b32  	%r50, %r40, 31;
	setp.eq.s32	%p20, %r50, 0;
	@%p20 bra 	BB6_30;

	mov.u32 	%r220, 32;
	sub.s32 	%r221, %r220, %r50;
	shr.u32 	%r222, %r365, %r221;
	shl.b32 	%r223, %r364, %r50;
	add.s32 	%r364, %r222, %r223;
	ld.local.u32 	%r224, [%rd12+-8];
	shr.u32 	%r225, %r224, %r221;
	shl.b32 	%r226, %r365, %r50;
	add.s32 	%r365, %r225, %r226;

BB6_30:
	shr.u32 	%r227, %r365, 30;
	shl.b32 	%r228, %r364, 2;
	add.s32 	%r366, %r227, %r228;
	shl.b32 	%r56, %r365, 2;
	shr.u32 	%r229, %r366, 31;
	shr.u32 	%r230, %r364, 30;
	add.s32 	%r57, %r229, %r230;
	setp.eq.s32	%p21, %r229, 0;
	mov.u32 	%r367, %r47;
	mov.u32 	%r368, %r56;
	@%p21 bra 	BB6_32;

	not.b32 	%r231, %r366;
	neg.s32 	%r58, %r56;
	setp.eq.s32	%p22, %r56, 0;
	selp.u32	%r232, 1, 0, %p22;
	add.s32 	%r366, %r232, %r231;
	xor.b32  	%r60, %r47, -2147483648;
	mov.u32 	%r367, %r60;
	mov.u32 	%r368, %r58;

BB6_32:
	mov.u32 	%r62, %r367;
	neg.s32 	%r233, %r57;
	setp.eq.s32	%p23, %r47, 0;
	selp.b32	%r371, %r57, %r233, %p23;
	clz.b32 	%r370, %r366;
	setp.eq.s32	%p24, %r370, 0;
	shl.b32 	%r234, %r366, %r370;
	mov.u32 	%r235, 32;
	sub.s32 	%r236, %r235, %r370;
	shr.u32 	%r237, %r368, %r236;
	add.s32 	%r238, %r237, %r234;
	selp.b32	%r66, %r366, %r238, %p24;
	mov.u32 	%r239, -921707870;
	mul.hi.u32 	%r369, %r66, %r239;
	setp.lt.s32	%p25, %r369, 1;
	@%p25 bra 	BB6_34;

	mul.lo.s32 	%r240, %r66, -921707870;
	shr.u32 	%r241, %r240, 31;
	shl.b32 	%r242, %r369, 1;
	add.s32 	%r369, %r241, %r242;
	add.s32 	%r370, %r370, 1;

BB6_34:
	mov.u32 	%r243, 126;
	sub.s32 	%r244, %r243, %r370;
	shl.b32 	%r245, %r244, 23;
	add.s32 	%r246, %r369, 1;
	shr.u32 	%r247, %r246, 7;
	add.s32 	%r248, %r247, 1;
	shr.u32 	%r249, %r248, 1;
	add.s32 	%r250, %r249, %r245;
	or.b32  	%r251, %r250, %r62;
	mov.b32 	 %f218, %r251;

BB6_35:
	mul.rn.f32 	%f29, %f218, %f218;
	and.b32  	%r73, %r371, 1;
	setp.eq.s32	%p26, %r73, 0;
	@%p26 bra 	BB6_37;

	mov.f32 	%f134, 0fBAB6061A;
	mov.f32 	%f135, 0f37CCF5CE;
	fma.rn.f32 	%f219, %f135, %f29, %f134;
	bra.uni 	BB6_38;

BB6_37:
	mov.f32 	%f136, 0f3C08839E;
	mov.f32 	%f137, 0fB94CA1F9;
	fma.rn.f32 	%f219, %f137, %f29, %f136;

BB6_38:
	@%p26 bra 	BB6_40;

	mov.f32 	%f138, 0f3D2AAAA5;
	fma.rn.f32 	%f139, %f219, %f29, %f138;
	mov.f32 	%f140, 0fBF000000;
	fma.rn.f32 	%f220, %f139, %f29, %f140;
	bra.uni 	BB6_41;

BB6_40:
	mov.f32 	%f141, 0fBE2AAAA3;
	fma.rn.f32 	%f142, %f219, %f29, %f141;
	mov.f32 	%f143, 0f00000000;
	fma.rn.f32 	%f220, %f142, %f29, %f143;

BB6_41:
	fma.rn.f32 	%f221, %f220, %f218, %f218;
	@%p26 bra 	BB6_43;

	mov.f32 	%f144, 0f3F800000;
	fma.rn.f32 	%f221, %f220, %f29, %f144;

BB6_43:
	and.b32  	%r252, %r371, 2;
	setp.eq.s32	%p29, %r252, 0;
	@%p29 bra 	BB6_45;

	mov.f32 	%f145, 0f00000000;
	mov.f32 	%f146, 0fBF800000;
	fma.rn.f32 	%f221, %f221, %f146, %f145;

BB6_45:
	mul.f32 	%f147, %f97, %f214;
	mul.f32 	%f148, %f98, %f221;
	sub.f32 	%f41, %f147, %f148;
	mul.f32 	%f149, %f97, %f221;
	fma.rn.f32 	%f42, %f98, %f214, %f149;
	neg.f32 	%f43, %f5;
	abs.f32 	%f44, %f43;
	setp.neu.f32	%p30, %f44, 0f7F800000;
	mov.f32 	%f228, %f43;
	@%p30 bra 	BB6_47;

	mov.f32 	%f150, 0f00000000;
	mul.rn.f32 	%f45, %f43, %f150;
	mov.f32 	%f228, %f45;

BB6_47:
	mov.f32 	%f46, %f228;
	mul.f32 	%f151, %f46, 0f3F22F983;
	cvt.rni.s32.f32	%r381, %f151;
	cvt.rn.f32.s32	%f152, %r381;
	neg.f32 	%f153, %f152;
	fma.rn.f32 	%f155, %f153, %f105, %f46;
	fma.rn.f32 	%f157, %f153, %f107, %f155;
	fma.rn.f32 	%f222, %f153, %f109, %f157;
	abs.f32 	%f159, %f46;
	setp.leu.f32	%p31, %f159, 0f47CE4780;
	@%p31 bra 	BB6_57;

	mov.b32 	 %r75, %f46;
	shr.u32 	%r76, %r75, 23;
	bfe.u32 	%r255, %r75, 23, 8;
	add.s32 	%r256, %r255, -128;
	shl.b32 	%r257, %r75, 8;
	or.b32  	%r77, %r257, -2147483648;
	shr.u32 	%r78, %r256, 5;
	add.u64 	%rd44, %SP, 0;
	cvta.to.local.u64 	%rd63, %rd44;
	mov.u32 	%r373, 0;
	mov.u64 	%rd62, __cudart_i2opi_f;
	mov.u32 	%r372, -6;

BB6_49:
	.pragma "nounroll";
	ld.const.u32 	%r260, [%rd62];
	// inline asm
	{
	mad.lo.cc.u32   %r258, %r260, %r77, %r373;
	madc.hi.u32     %r373, %r260, %r77,  0;
	}
	// inline asm
	st.local.u32 	[%rd63], %r258;
	add.s64 	%rd63, %rd63, 4;
	add.s64 	%rd62, %rd62, 4;
	add.s32 	%r372, %r372, 1;
	setp.ne.s32	%p32, %r372, 0;
	@%p32 bra 	BB6_49;

	and.b32  	%r83, %r75, -2147483648;
	cvta.to.local.u64 	%rd46, %rd44;
	st.local.u32 	[%rd46+24], %r373;
	mov.u32 	%r263, 6;
	sub.s32 	%r264, %r263, %r78;
	mul.wide.s32 	%rd47, %r264, 4;
	add.s64 	%rd18, %rd46, %rd47;
	ld.local.u32 	%r374, [%rd18];
	ld.local.u32 	%r375, [%rd18+-4];
	and.b32  	%r86, %r76, 31;
	setp.eq.s32	%p33, %r86, 0;
	@%p33 bra 	BB6_52;

	mov.u32 	%r265, 32;
	sub.s32 	%r266, %r265, %r86;
	shr.u32 	%r267, %r375, %r266;
	shl.b32 	%r268, %r374, %r86;
	add.s32 	%r374, %r267, %r268;
	ld.local.u32 	%r269, [%rd18+-8];
	shr.u32 	%r270, %r269, %r266;
	shl.b32 	%r271, %r375, %r86;
	add.s32 	%r375, %r270, %r271;

BB6_52:
	shr.u32 	%r272, %r375, 30;
	shl.b32 	%r273, %r374, 2;
	add.s32 	%r376, %r272, %r273;
	shl.b32 	%r92, %r375, 2;
	shr.u32 	%r274, %r376, 31;
	shr.u32 	%r275, %r374, 30;
	add.s32 	%r93, %r274, %r275;
	setp.eq.s32	%p34, %r274, 0;
	mov.u32 	%r377, %r83;
	mov.u32 	%r378, %r92;
	@%p34 bra 	BB6_54;

	not.b32 	%r276, %r376;
	neg.s32 	%r94, %r92;
	setp.eq.s32	%p35, %r92, 0;
	selp.u32	%r277, 1, 0, %p35;
	add.s32 	%r376, %r277, %r276;
	xor.b32  	%r96, %r83, -2147483648;
	mov.u32 	%r377, %r96;
	mov.u32 	%r378, %r94;

BB6_54:
	mov.u32 	%r98, %r377;
	neg.s32 	%r278, %r93;
	setp.eq.s32	%p36, %r83, 0;
	selp.b32	%r381, %r93, %r278, %p36;
	clz.b32 	%r380, %r376;
	setp.eq.s32	%p37, %r380, 0;
	shl.b32 	%r279, %r376, %r380;
	mov.u32 	%r280, 32;
	sub.s32 	%r281, %r280, %r380;
	shr.u32 	%r282, %r378, %r281;
	add.s32 	%r283, %r282, %r279;
	selp.b32	%r102, %r376, %r283, %p37;
	mov.u32 	%r284, -921707870;
	mul.hi.u32 	%r379, %r102, %r284;
	setp.lt.s32	%p38, %r379, 1;
	@%p38 bra 	BB6_56;

	mul.lo.s32 	%r285, %r102, -921707870;
	shr.u32 	%r286, %r285, 31;
	shl.b32 	%r287, %r379, 1;
	add.s32 	%r379, %r286, %r287;
	add.s32 	%r380, %r380, 1;

BB6_56:
	mov.u32 	%r288, 126;
	sub.s32 	%r289, %r288, %r380;
	shl.b32 	%r290, %r289, 23;
	add.s32 	%r291, %r379, 1;
	shr.u32 	%r292, %r291, 7;
	add.s32 	%r293, %r292, 1;
	shr.u32 	%r294, %r293, 1;
	add.s32 	%r295, %r294, %r290;
	or.b32  	%r296, %r295, %r98;
	mov.b32 	 %f222, %r296;

BB6_57:
	mul.rn.f32 	%f50, %f222, %f222;
	add.s32 	%r109, %r381, 1;
	and.b32  	%r110, %r109, 1;
	setp.eq.s32	%p39, %r110, 0;
	@%p39 bra 	BB6_59;

	mov.f32 	%f160, 0fBAB6061A;
	mov.f32 	%f161, 0f37CCF5CE;
	fma.rn.f32 	%f223, %f161, %f50, %f160;
	bra.uni 	BB6_60;

BB6_59:
	mov.f32 	%f162, 0f3C08839E;
	mov.f32 	%f163, 0fB94CA1F9;
	fma.rn.f32 	%f223, %f163, %f50, %f162;

BB6_60:
	@%p39 bra 	BB6_62;

	mov.f32 	%f164, 0f3D2AAAA5;
	fma.rn.f32 	%f165, %f223, %f50, %f164;
	mov.f32 	%f166, 0fBF000000;
	fma.rn.f32 	%f224, %f165, %f50, %f166;
	bra.uni 	BB6_63;

BB6_62:
	mov.f32 	%f167, 0fBE2AAAA3;
	fma.rn.f32 	%f168, %f223, %f50, %f167;
	mov.f32 	%f169, 0f00000000;
	fma.rn.f32 	%f224, %f168, %f50, %f169;

BB6_63:
	fma.rn.f32 	%f225, %f224, %f222, %f222;
	@%p39 bra 	BB6_65;

	mov.f32 	%f170, 0f3F800000;
	fma.rn.f32 	%f225, %f224, %f50, %f170;

BB6_65:
	and.b32  	%r297, %r109, 2;
	setp.eq.s32	%p42, %r297, 0;
	@%p42 bra 	BB6_67;

	mov.f32 	%f171, 0f00000000;
	mov.f32 	%f172, 0fBF800000;
	fma.rn.f32 	%f225, %f225, %f172, %f171;

BB6_67:
	mov.f32 	%f227, %f43;
	@%p30 bra 	BB6_69;

	mov.f32 	%f173, 0f00000000;
	mul.rn.f32 	%f227, %f43, %f173;

BB6_69:
	mul.f32 	%f174, %f227, 0f3F22F983;
	cvt.rni.s32.f32	%r391, %f174;
	cvt.rn.f32.s32	%f175, %r391;
	neg.f32 	%f176, %f175;
	fma.rn.f32 	%f178, %f176, %f105, %f227;
	fma.rn.f32 	%f180, %f176, %f107, %f178;
	fma.rn.f32 	%f229, %f176, %f109, %f180;
	abs.f32 	%f182, %f227;
	setp.leu.f32	%p44, %f182, 0f47CE4780;
	@%p44 bra 	BB6_79;

	mov.b32 	 %r112, %f227;
	shr.u32 	%r113, %r112, 23;
	bfe.u32 	%r300, %r112, 23, 8;
	add.s32 	%r301, %r300, -128;
	shl.b32 	%r302, %r112, 8;
	or.b32  	%r114, %r302, -2147483648;
	shr.u32 	%r115, %r301, 5;
	add.u64 	%rd49, %SP, 0;
	cvta.to.local.u64 	%rd65, %rd49;
	mov.u32 	%r383, 0;
	mov.u64 	%rd64, __cudart_i2opi_f;
	mov.u32 	%r382, -6;

BB6_71:
	.pragma "nounroll";
	ld.const.u32 	%r305, [%rd64];
	// inline asm
	{
	mad.lo.cc.u32   %r303, %r305, %r114, %r383;
	madc.hi.u32     %r383, %r305, %r114,  0;
	}
	// inline asm
	st.local.u32 	[%rd65], %r303;
	add.s64 	%rd65, %rd65, 4;
	add.s64 	%rd64, %rd64, 4;
	add.s32 	%r382, %r382, 1;
	setp.ne.s32	%p45, %r382, 0;
	@%p45 bra 	BB6_71;

	and.b32  	%r120, %r112, -2147483648;
	cvta.to.local.u64 	%rd51, %rd49;
	st.local.u32 	[%rd51+24], %r383;
	mov.u32 	%r308, 6;
	sub.s32 	%r309, %r308, %r115;
	mul.wide.s32 	%rd52, %r309, 4;
	add.s64 	%rd24, %rd51, %rd52;
	ld.local.u32 	%r384, [%rd24];
	ld.local.u32 	%r385, [%rd24+-4];
	and.b32  	%r123, %r113, 31;
	setp.eq.s32	%p46, %r123, 0;
	@%p46 bra 	BB6_74;

	mov.u32 	%r310, 32;
	sub.s32 	%r311, %r310, %r123;
	shr.u32 	%r312, %r385, %r311;
	shl.b32 	%r313, %r384, %r123;
	add.s32 	%r384, %r312, %r313;
	ld.local.u32 	%r314, [%rd24+-8];
	shr.u32 	%r315, %r314, %r311;
	shl.b32 	%r316, %r385, %r123;
	add.s32 	%r385, %r315, %r316;

BB6_74:
	shr.u32 	%r317, %r385, 30;
	shl.b32 	%r318, %r384, 2;
	add.s32 	%r386, %r317, %r318;
	shl.b32 	%r129, %r385, 2;
	shr.u32 	%r319, %r386, 31;
	shr.u32 	%r320, %r384, 30;
	add.s32 	%r130, %r319, %r320;
	setp.eq.s32	%p47, %r319, 0;
	mov.u32 	%r387, %r120;
	mov.u32 	%r388, %r129;
	@%p47 bra 	BB6_76;

	not.b32 	%r321, %r386;
	neg.s32 	%r131, %r129;
	setp.eq.s32	%p48, %r129, 0;
	selp.u32	%r322, 1, 0, %p48;
	add.s32 	%r386, %r322, %r321;
	xor.b32  	%r133, %r120, -2147483648;
	mov.u32 	%r387, %r133;
	mov.u32 	%r388, %r131;

BB6_76:
	mov.u32 	%r135, %r387;
	neg.s32 	%r323, %r130;
	setp.eq.s32	%p49, %r120, 0;
	selp.b32	%r391, %r130, %r323, %p49;
	clz.b32 	%r390, %r386;
	setp.eq.s32	%p50, %r390, 0;
	shl.b32 	%r324, %r386, %r390;
	mov.u32 	%r325, 32;
	sub.s32 	%r326, %r325, %r390;
	shr.u32 	%r327, %r388, %r326;
	add.s32 	%r328, %r327, %r324;
	selp.b32	%r139, %r386, %r328, %p50;
	mov.u32 	%r329, -921707870;
	mul.hi.u32 	%r389, %r139, %r329;
	setp.lt.s32	%p51, %r389, 1;
	@%p51 bra 	BB6_78;

	mul.lo.s32 	%r330, %r139, -921707870;
	shr.u32 	%r331, %r330, 31;
	shl.b32 	%r332, %r389, 1;
	add.s32 	%r389, %r331, %r332;
	add.s32 	%r390, %r390, 1;

BB6_78:
	mov.u32 	%r333, 126;
	sub.s32 	%r334, %r333, %r390;
	shl.b32 	%r335, %r334, 23;
	add.s32 	%r336, %r389, 1;
	shr.u32 	%r337, %r336, 7;
	add.s32 	%r338, %r337, 1;
	shr.u32 	%r339, %r338, 1;
	add.s32 	%r340, %r339, %r335;
	or.b32  	%r341, %r340, %r135;
	mov.b32 	 %f229, %r341;

BB6_79:
	mul.rn.f32 	%f67, %f229, %f229;
	and.b32  	%r146, %r391, 1;
	setp.eq.s32	%p52, %r146, 0;
	@%p52 bra 	BB6_81;

	mov.f32 	%f183, 0fBAB6061A;
	mov.f32 	%f184, 0f37CCF5CE;
	fma.rn.f32 	%f230, %f184, %f67, %f183;
	bra.uni 	BB6_82;

BB6_81:
	mov.f32 	%f185, 0f3C08839E;
	mov.f32 	%f186, 0fB94CA1F9;
	fma.rn.f32 	%f230, %f186, %f67, %f185;

BB6_82:
	@%p52 bra 	BB6_84;

	mov.f32 	%f187, 0f3D2AAAA5;
	fma.rn.f32 	%f188, %f230, %f67, %f187;
	mov.f32 	%f189, 0fBF000000;
	fma.rn.f32 	%f231, %f188, %f67, %f189;
	bra.uni 	BB6_85;

BB6_84:
	mov.f32 	%f190, 0fBE2AAAA3;
	fma.rn.f32 	%f191, %f230, %f67, %f190;
	mov.f32 	%f192, 0f00000000;
	fma.rn.f32 	%f231, %f191, %f67, %f192;

BB6_85:
	fma.rn.f32 	%f232, %f231, %f229, %f229;
	@%p52 bra 	BB6_87;

	mov.f32 	%f193, 0f3F800000;
	fma.rn.f32 	%f232, %f231, %f67, %f193;

BB6_87:
	and.b32  	%r342, %r391, 2;
	setp.eq.s32	%p55, %r342, 0;
	@%p55 bra 	BB6_89;

	mov.f32 	%f194, 0f00000000;
	mov.f32 	%f195, 0fBF800000;
	fma.rn.f32 	%f232, %f232, %f195, %f194;

BB6_89:
	mul.f32 	%f196, %f232, %f100;
	fma.rn.f32 	%f197, %f99, %f225, %f196;
	mul.f32 	%f198, %f99, %f232;
	mul.f32 	%f199, %f225, %f100;
	sub.f32 	%f200, %f198, %f199;
	add.f32 	%f201, %f41, %f197;
	add.f32 	%f202, %f42, %f200;
	mad.lo.s32 	%r351, %r2, %r148, %r1;
	cvta.to.global.u64 	%rd53, %rd27;
	mul.wide.u32 	%rd54, %r351, 8;
	add.s64 	%rd55, %rd53, %rd54;
	ld.global.v2.f32 	{%f203, %f204}, [%rd55];
	sub.f32 	%f207, %f203, %f201;
	sub.f32 	%f208, %f204, %f202;
	cvta.to.global.u64 	%rd56, %rd26;
	add.s64 	%rd57, %rd56, %rd54;
	fma.rn.f32 	%f209, %f208, %f80, %f202;
	fma.rn.f32 	%f210, %f207, %f80, %f201;
	st.global.v2.f32 	[%rd57], {%f210, %f209};

BB6_90:
	ret;
}

	// .globl	updateHeightmapKernel
.visible .entry updateHeightmapKernel(
	.param .u64 updateHeightmapKernel_param_0,
	.param .u64 updateHeightmapKernel_param_1,
	.param .u32 updateHeightmapKernel_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [updateHeightmapKernel_param_0];
	ld.param.u64 	%rd2, [updateHeightmapKernel_param_1];
	ld.param.u32 	%r1, [updateHeightmapKernel_param_2];
	cvta.to.global.u64 	%rd3, %rd1;
	cvta.to.global.u64 	%rd4, %rd2;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r5, %r3, %r2, %r4;
	mov.u32 	%r6, %ctaid.y;
	mov.u32 	%r7, %ntid.y;
	mov.u32 	%r8, %tid.y;
	mad.lo.s32 	%r9, %r7, %r6, %r8;
	mad.lo.s32 	%r10, %r9, %r1, %r5;
	add.s32 	%r11, %r9, %r5;
	and.b32  	%r12, %r11, 1;
	setp.eq.b32	%p1, %r12, 1;
	not.pred 	%p2, %p1;
	selp.f32	%f1, 0f3F800000, 0fBF800000, %p2;
	mul.wide.u32 	%rd5, %r10, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f2, [%rd6];
	mul.f32 	%f3, %f2, %f1;
	mul.wide.u32 	%rd7, %r10, 4;
	add.s64 	%rd8, %rd3, %rd7;
	st.global.f32 	[%rd8], %f3;
	ret;
}

	// .globl	calculateSlopeKernel
.visible .entry calculateSlopeKernel(
	.param .u64 calculateSlopeKernel_param_0,
	.param .u64 calculateSlopeKernel_param_1,
	.param .u32 calculateSlopeKernel_param_2,
	.param .u32 calculateSlopeKernel_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd1, [calculateSlopeKernel_param_0];
	ld.param.u64 	%rd2, [calculateSlopeKernel_param_1];
	ld.param.u32 	%r2, [calculateSlopeKernel_param_2];
	ld.param.u32 	%r3, [calculateSlopeKernel_param_3];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r7, %r4, %r5, %r6;
	mov.u32 	%r8, %ntid.y;
	mov.u32 	%r9, %ctaid.y;
	mov.u32 	%r10, %tid.y;
	mad.lo.s32 	%r11, %r8, %r9, %r10;
	mad.lo.s32 	%r1, %r11, %r2, %r7;
	setp.ne.s32	%p1, %r11, 0;
	setp.ne.s32	%p2, %r7, 0;
	and.pred  	%p3, %p1, %p2;
	add.s32 	%r12, %r2, -1;
	setp.lt.u32	%p4, %r7, %r12;
	and.pred  	%p5, %p3, %p4;
	add.s32 	%r13, %r3, -1;
	setp.lt.u32	%p6, %r11, %r13;
	and.pred  	%p7, %p5, %p6;
	mov.f32 	%f12, 0f00000000;
	mov.f32 	%f11, %f12;
	@!%p7 bra 	BB8_2;
	bra.uni 	BB8_1;

BB8_1:
	cvta.to.global.u64 	%rd3, %rd1;
	add.s32 	%r14, %r1, 1;
	mul.wide.u32 	%rd4, %r14, 4;
	add.s64 	%rd5, %rd3, %rd4;
	add.s32 	%r15, %r1, -1;
	mul.wide.u32 	%rd6, %r15, 4;
	add.s64 	%rd7, %rd3, %rd6;
	ld.global.f32 	%f7, [%rd7];
	ld.global.f32 	%f8, [%rd5];
	sub.f32 	%f12, %f8, %f7;
	add.s32 	%r16, %r1, %r2;
	mul.wide.u32 	%rd8, %r16, 4;
	add.s64 	%rd9, %rd3, %rd8;
	sub.s32 	%r17, %r1, %r2;
	mul.wide.u32 	%rd10, %r17, 4;
	add.s64 	%rd11, %rd3, %rd10;
	ld.global.f32 	%f9, [%rd11];
	ld.global.f32 	%f10, [%rd9];
	sub.f32 	%f11, %f10, %f9;

BB8_2:
	cvta.to.global.u64 	%rd12, %rd2;
	mul.wide.u32 	%rd13, %r1, 8;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.v2.f32 	[%rd14], {%f12, %f11};
	ret;
}


